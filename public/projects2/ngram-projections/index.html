<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>ngram-projections | metafunctor</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Stars: 0 | Forks: 0 | Open Issues: 0
Languages: Python, Jupyter Notebook
Contributors

queelius (5 commits)

README
Autoregressive Models: Inductive Biases and Projections
This paper investigates how inductive biases, particularly projections onto training data, can be utilized in autoregressive (AR) models to improve out-of-distribution (OOD) generalization.
Our approach consists of the following steps:


Infini-gram Model Exploration: We begin by considering the infinite-gram model, which uses suffix arrays to efficiently handle arbitrary input (context) lengths. Infinite-gram models are noteworthy due to their ability to scale to any Markov order. For reference, see the paper Infini-gram: An Engine for n-gram / ∞-gram Language Modeling with Trillion-Token Corpora by Jiacheng Liu et al. (https://huggingface.co/papers/2401.17377). A brute-force approach could involve storing the training data as-is and searching for the longest matching suffix in the input sequence.">
    <meta name="generator" content="Hugo 0.139.2">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Alex Towell">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>
        
    
    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/projects2/ngram-projections/">
    

    <meta property="og:url" content="http://localhost:1313/projects2/ngram-projections/">
  <meta property="og:site_name" content="metafunctor">
  <meta property="og:title" content="ngram-projections">
  <meta property="og:description" content="Stars: 0 | Forks: 0 | Open Issues: 0
Languages: Python, Jupyter Notebook
Contributors queelius (5 commits) README Autoregressive Models: Inductive Biases and Projections This paper investigates how inductive biases, particularly projections onto training data, can be utilized in autoregressive (AR) models to improve out-of-distribution (OOD) generalization.
Our approach consists of the following steps:
Infini-gram Model Exploration: We begin by considering the infinite-gram model, which uses suffix arrays to efficiently handle arbitrary input (context) lengths. Infinite-gram models are noteworthy due to their ability to scale to any Markov order. For reference, see the paper Infini-gram: An Engine for n-gram / ∞-gram Language Modeling with Trillion-Token Corpora by Jiacheng Liu et al. (https://huggingface.co/papers/2401.17377). A brute-force approach could involve storing the training data as-is and searching for the longest matching suffix in the input sequence.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects2">
    <meta property="article:published_time" content="2024-06-21T11:50:22+00:00">
    <meta property="article:modified_time" content="2024-06-21T11:50:22+00:00">
    <meta property="article:tag" content="GitHub">
    <meta property="article:tag" content="Project">

  <meta itemprop="name" content="ngram-projections">
  <meta itemprop="description" content="Stars: 0 | Forks: 0 | Open Issues: 0
Languages: Python, Jupyter Notebook
Contributors queelius (5 commits) README Autoregressive Models: Inductive Biases and Projections This paper investigates how inductive biases, particularly projections onto training data, can be utilized in autoregressive (AR) models to improve out-of-distribution (OOD) generalization.
Our approach consists of the following steps:
Infini-gram Model Exploration: We begin by considering the infinite-gram model, which uses suffix arrays to efficiently handle arbitrary input (context) lengths. Infinite-gram models are noteworthy due to their ability to scale to any Markov order. For reference, see the paper Infini-gram: An Engine for n-gram / ∞-gram Language Modeling with Trillion-Token Corpora by Jiacheng Liu et al. (https://huggingface.co/papers/2401.17377). A brute-force approach could involve storing the training data as-is and searching for the longest matching suffix in the input sequence.">
  <meta itemprop="datePublished" content="2024-06-21T11:50:22+00:00">
  <meta itemprop="dateModified" content="2024-06-21T11:50:22+00:00">
  <meta itemprop="wordCount" content="366">
  <meta itemprop="keywords" content="GitHub,Project">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="ngram-projections">
  <meta name="twitter:description" content="Stars: 0 | Forks: 0 | Open Issues: 0
Languages: Python, Jupyter Notebook
Contributors queelius (5 commits) README Autoregressive Models: Inductive Biases and Projections This paper investigates how inductive biases, particularly projections onto training data, can be utilized in autoregressive (AR) models to improve out-of-distribution (OOD) generalization.
Our approach consists of the following steps:
Infini-gram Model Exploration: We begin by considering the infinite-gram model, which uses suffix arrays to efficiently handle arbitrary input (context) lengths. Infinite-gram models are noteworthy due to their ability to scale to any Markov order. For reference, see the paper Infini-gram: An Engine for n-gram / ∞-gram Language Modeling with Trillion-Token Corpora by Jiacheng Liu et al. (https://huggingface.co/papers/2401.17377). A brute-force approach could involve storing the training data as-is and searching for the longest matching suffix in the input sequence.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        metafunctor
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/social/" title="Social page">
              Social
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/ghprojects/" title="GitHub Projects page">
              GitHub Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="News page">
              News
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/probsets/" title="Problem Sets page">
              Problem Sets
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects2/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/research/" title="Research projects page">
              Research projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/search/" title="Search page">
              Search
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Projects
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">ngram-projections</h1>
      
      <p class="tracked">
        By <strong>Alex Towell</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-06-21T11:50:22Z">June 21, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><strong>Stars</strong>: 0 | <strong>Forks</strong>: 0 | <strong>Open Issues</strong>: 0</p>
<p><strong>Languages</strong>: Python, Jupyter Notebook</p>
<h2 id="contributors">Contributors</h2>
<ul>
<li>queelius (5 commits)</li>
</ul>
<h2 id="readme">README</h2>
<h1 id="autoregressive-models-inductive-biases-and-projections">Autoregressive Models: Inductive Biases and Projections</h1>
<p>This paper investigates how inductive biases, particularly projections onto training data, can be utilized in autoregressive (AR) models to improve out-of-distribution (OOD) generalization.</p>
<p>Our approach consists of the following steps:</p>
<ol>
<li>
<p><strong>Infini-gram Model Exploration</strong>: We begin by considering the infinite-gram model, which uses suffix arrays to efficiently handle arbitrary input (context) lengths. Infinite-gram models are noteworthy due to their ability to scale to any Markov order. For reference, see the paper <em>Infini-gram: An Engine for n-gram / ∞-gram Language Modeling with Trillion-Token Corpora</em> by Jiacheng Liu et al. (<a href="https://huggingface.co/papers/2401.17377)">https://huggingface.co/papers/2401.17377)</a>. A brute-force approach could involve storing the training data as-is and searching for the longest matching suffix in the input sequence.</p>
</li>
<li>
<p><strong>Projection-Based Inductive Biases</strong>: We explore a subset of inductive biases where inputs are projected onto the training data. Additionally, we consider mapping output tokens from the training data back onto the context to maintain coherence. For example, if the input is &ldquo;the king demanded&rdquo; and &ldquo;king&rdquo; is substituted with &ldquo;dictator,&rdquo; we could implement a token mapping to revert &ldquo;dictator&rdquo; to &ldquo;king&rdquo; in subsequent outputs. However, this approach introduces potential challenges in maintaining consistency.</p>
</li>
<li>
<p><strong>Exploring Variations and Extensions</strong>: In the default implementation, the model finds the longest matching suffix, exhibiting a recency bias. However, there are many possible variations to consider. Given the rich history of n-gram models, revisiting these concepts in the context of Infini-gram models and modern LLMs is worthwhile. While we do not expect to match the OOD generalization capabilities of models like GPT, we anticipate developing more sophisticated projection functions that surpass the simple longest-suffix matching approach.</p>
</li>
</ol>
<p>By framing OOD generalization as a projection problem, we propose strategies to optimize these projections using machine learning techniques. Additionally, we explore the integration of classical information retrieval methods and pre-trained language model embeddings to improve the semantic relevance of these projections.</p>
<p>We are also developing code for a Python library to experiment with various inductive biases. This library will enable running these simpler models alongside more powerful neural generative models like GPTs, where these models &ldquo;learn&rdquo; by simply accumulating more training data.</p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/github/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">GitHub</a>
   </li>
  
   <li class="list di">
     <a href="/tags/project/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Project</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "metafunctor-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/ghprojects/ngram-projections/">ngram-projections</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/treeprog/">treeprog</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/treeprog/">treeprog</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/random_oracles/">random_oracles</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/random_oracles/">random_oracles</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/ollama_data_tools/">ollama_data_tools</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/ollama_data_tools/">ollama_data_tools</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/algotree/">AlgoTree</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/algotree/">AlgoTree</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/hypothesize/">hypothesize</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/hypothesize/">hypothesize</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/solomonoff_induction/">solomonoff_induction</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/solomonoff_induction/">solomonoff_induction</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/ghprojects/sluug-talk-llm/">sluug-talk-llm</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/projects2/sluug-talk-llm/">sluug-talk-llm</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  metafunctor 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
