<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=42999&amp;path=livereload" data-no-instant defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>RPSDG | metafunctor</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="RPSDG
GitHub Link
Stars: 1 | Forks: 0 | Open Issues: 0
Languages Used: Python, JavaScript, Jupyter Notebook, CSS, HTML, Mathematica, Dockerfile
README
Reverse-Process Synthetic Data Generation: Automatically Generating Training Language Models for Complex Problem Solving
Abstract:
This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed &ldquo;Reverse-Process Synthetic Data Generation&rdquo; (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions,
e.g., symbolically taking the deriative of a function, $f \mapsto f&rsquo;$, versus solving antiderivatives of $f&rsquo;$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs&rsquo; problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.">
    <meta name="generator" content="Hugo 0.139.2">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Alex Towell">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>
        
    
    
      

    

    

    
      <link rel="canonical" href="http://192.168.0.225:42999/ghprojects/rpsdg/">
    

    <meta property="og:url" content="http://192.168.0.225:42999/ghprojects/rpsdg/">
  <meta property="og:site_name" content="metafunctor">
  <meta property="og:title" content="RPSDG">
  <meta property="og:description" content="RPSDG GitHub Link Stars: 1 | Forks: 0 | Open Issues: 0 Languages Used: Python, JavaScript, Jupyter Notebook, CSS, HTML, Mathematica, Dockerfile
README Reverse-Process Synthetic Data Generation: Automatically Generating Training Language Models for Complex Problem Solving Abstract: This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed “Reverse-Process Synthetic Data Generation” (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions, e.g., symbolically taking the deriative of a function, $f \mapsto f’$, versus solving antiderivatives of $f’$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs’ problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ghprojects">
    <meta property="article:published_time" content="2024-07-25T16:24:45+00:00">
    <meta property="article:modified_time" content="2024-07-25T16:24:45+00:00">
    <meta property="article:tag" content="GitHub">
    <meta property="article:tag" content="Project">

  <meta itemprop="name" content="RPSDG">
  <meta itemprop="description" content="RPSDG GitHub Link Stars: 1 | Forks: 0 | Open Issues: 0 Languages Used: Python, JavaScript, Jupyter Notebook, CSS, HTML, Mathematica, Dockerfile
README Reverse-Process Synthetic Data Generation: Automatically Generating Training Language Models for Complex Problem Solving Abstract: This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed “Reverse-Process Synthetic Data Generation” (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions, e.g., symbolically taking the deriative of a function, $f \mapsto f’$, versus solving antiderivatives of $f’$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs’ problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.">
  <meta itemprop="datePublished" content="2024-07-25T16:24:45+00:00">
  <meta itemprop="dateModified" content="2024-07-25T16:24:45+00:00">
  <meta itemprop="wordCount" content="1064">
  <meta itemprop="keywords" content="GitHub,Project">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RPSDG">
  <meta name="twitter:description" content="RPSDG GitHub Link Stars: 1 | Forks: 0 | Open Issues: 0 Languages Used: Python, JavaScript, Jupyter Notebook, CSS, HTML, Mathematica, Dockerfile
README Reverse-Process Synthetic Data Generation: Automatically Generating Training Language Models for Complex Problem Solving Abstract: This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed “Reverse-Process Synthetic Data Generation” (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions, e.g., symbolically taking the deriative of a function, $f \mapsto f’$, versus solving antiderivatives of $f’$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs’ problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        metafunctor
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/social/" title="Social page">
              Social
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/ghprojects/" title="GitHub Projects page">
              GitHub Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="News page">
              News
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/probsets/" title="Problem Sets page">
              Problem Sets
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects-orig/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/research/" title="Research projects page">
              Research projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/search/" title="Search page">
              Search
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
<section class="project-section" style="padding: 2rem; background-color: #f9f9f9;">
  <div class="container" style="max-width: 1200px; margin: 0 auto;">
    
    <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 2rem;">
      
      
      <div>
        
        <header style="margin-bottom: 2rem;">
          <h1 style="font-size: 2.5rem; font-weight: 700; color: #333;">RPSDG</h1>
          <p style="color: #555; font-size: 1.1rem; margin-bottom: 0.5rem;">
             | July 25, 2024 | <a href="mailto:" style="color: #007acc;"></a>
          </p>

          
          <div style="overflow: hidden;">
            
            <p style="font-size: 1.25rem; color: #666;"></p>
          </div>
        </header>

        
        
        <section style="margin-bottom: 1rem;">
          <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
            
            <a href="https://github.com/queelius/RPSDG" target="_blank" style="padding: 0.75rem 1.5rem; background-color: #007acc; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
              GitHub
            </a>
            
            
            <a href="https://twitter.com/intent/tweet?text=rpsdg&url=http%3a%2f%2f192.168.0.225%3a42999%2fghprojects%2frpsdg%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #1DA1F2; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-twitter"></i>
            </a>

            
            <a href="https://www.linkedin.com/sharing/share-offsite/?url=http%3a%2f%2f192.168.0.225%3a42999%2fghprojects%2frpsdg%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #0077B5; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-linkedin"></i>
            </a>

            
            <a href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2f192.168.0.225%3a42999%2fghprojects%2frpsdg%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #1877F2; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-facebook"></i>
            </a>
          </div>
        </section>
        

        
        <section style="margin-bottom: 3rem;">
          <h2 style="font-size: 1.75rem; font-weight: 600; color: #007acc;">Project Description</h2>
          <p style="font-size: 1.1rem; color: #555; line-height: 1.8;">
            <h1 id="rpsdg">RPSDG</h1>
<p><a href="https://github.com/queelius/RPSDG">GitHub Link</a>
<strong>Stars</strong>: 1 | <strong>Forks</strong>: 0 | <strong>Open Issues</strong>: 0
<strong>Languages Used</strong>: Python, JavaScript, Jupyter Notebook, CSS, HTML, Mathematica, Dockerfile</p>
<h2 id="readme">README</h2>
<h1 id="reverse-process-synthetic-data-generation-automatically-generating-training-language-models-for-complex-problem-solving">Reverse-Process Synthetic Data Generation: Automatically Generating Training Language Models for Complex Problem Solving</h1>
<h2 id="abstract">Abstract:</h2>
<p>This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed &ldquo;Reverse-Process Synthetic Data Generation&rdquo; (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions,
e.g., symbolically taking the deriative of a function, $f \mapsto f&rsquo;$, versus solving antiderivatives of $f&rsquo;$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs&rsquo; problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.</p>
<h2 id="table-of-contents">Table of Contents:</h2>
<ul>
<li>
<p>Introduction</p>
<ul>
<li>The challenge of training data for complex problem-solving</li>
<li>Overview of Reverse-Process Synthetic Data Generation (RPSDG)</li>
<li>Potential impact on AI capabilities and explainability</li>
</ul>
</li>
<li>
<p>Methodology</p>
<ul>
<li>Core principles</li>
<li>Automating generation of process supervision training data</li>
<li>Curriculum learning and problem difficulty progression</li>
</ul>
</li>
<li>
<p>Mathematics</p>
<ul>
<li>Algebra: Equation solving and manipulation</li>
<li>Calculus: From differentiation to integration</li>
</ul>
</li>
<li>
<p>Implementation and Results</p>
<ul>
<li>Data generation pipelines</li>
<li>Transformer-based LMs</li>
<li>Self-Supervised Learning</li>
<li>Evals and benchmarks</li>
</ul>
</li>
<li>
<p>Discussion</p>
<ul>
<li>Implications for AI problem-solving capabilities</li>
<li>Enhancing explainability and transparency</li>
<li>Limitations and challenges of the RPSDG approach using SSL</li>
</ul>
</li>
<li>
<p>Future Work</p>
<ul>
<li>Expanding to new domains and problem types</li>
<li>Reinforcement learning to reward multi-step reasoning even without a known (but verifiable) solution</li>
</ul>
</li>
<li>
<p>Conclusion</p>
<ul>
<li>Summary of key findings</li>
<li>Broader impact on AI research and applications</li>
</ul>
</li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>In &ldquo;The Bitter Lesson,&rdquo; Richard Sutton argues that learning algorithms that scale with compute and data will eventually outperform handcrafted algorithms.</p>
<p>The next frontier in AI research is finding ways to acquire high-quality data that can be used to train models to predict the latent structure and processes in the world. A significant portion of the world&rsquo;s data is latent, where the processes that generate the data are not observable (e.g., not written down). For example, in mathematics, the way in which a proof was discovered is often not demonstrated and instead only a polished proof is presented, hiding the creative process and the &ldquo;dark matter&rdquo; that led to the proof. Understanding and modeling the latent structure in our processes can lead to significant improvements in AI capabilities.</p>
<p>In this paper, we are interested in exploring <em>algorithmic</em> data generation, where we apply classical algorithms (GOFAI) to automatically generate high-quality step-by-step (process supervision) training data for LMs. In particular, we are interested in exploring problems which have the feature of being easy to solve in one direction, but hard to solve in the other direction, such as</p>
<h2 id="taking-derivatives-vs-integrating-functions">Taking Derivatives vs. Integrating Functions</h2>
<p>In mathematics, computing derivatives of functions is generally easier than finding their antiderivatives (integrals). This inherent asymmetry allows us to use the more straightforward differentiation process to generate a rich dataset for training language models (LLMs) by reversing the problem-solving direction: starting with derivatives and deriving the original functions.</p>
<h3 id="generating-integral-calculus-training-data-by-solving-derivatives-and-reversing-the-process">Generating Integral Calculus Training Data By Solving Derivatives and Reversing the Process</h3>
<ol>
<li>
<p><strong>Starting with Known Functions:</strong></p>
<ul>
<li>Select functions \( f(x) \) that have closed-form solutions and well-defined derivatives.</li>
<li>Examples include polynomials, trigonometric functions, exponential functions, and logarithmic functions.</li>
<li>To ensure the training data covers a wide range of functions and their transformations, we create a variety of functions with different complexities and forms.</li>
</ul>
</li>
<li>
<p><strong>Formulating the Reverse Process:</strong></p>
<ul>
<li>Take the derivative of \( f(x) \) to generate \( f'(x) \).</li>
</ul>
</li>
<li>
<p><strong>Reversing the Process:</strong></p>
<ul>
<li>Generate the RPSDG by reversing the problem and solution steps, starting with \( f'(x) \)$ to show how to arrive at a corresponding integral \( f(x) \).</li>
<li>Ensure that each step in this process is well-documented, capturing the intermediate transformations.</li>
</ul>
</li>
</ol>
<p>This approach allows us to generate integration problems of graduating difficulty, leveraging the inherent asymmetry between differentiation and integration. By automating this process, we create a diverse dataset for training LLMs to predict solutions in integral calculus.</p>
<h2 id="generating-a-theorem-proof-vs-verifying-a-proof">Generating a Theorem Proof vs. Verifying a Proof</h2>
<p>One of the key challenges in mathematics and logic is generating proofs for given theorems. While verifying a proof is generally straightforward, generating the proof itself can be significantly more complex. Our methodology leverages this asymmetry by focusing on the reverse process: starting with randomly generated expressions and using rewrite rules to create both theorems and their proofs.</p>
<h3 id="generating-theorem-proofs">Generating Theorem Proofs</h3>
<ol>
<li>
<p><strong>Random Walks in Expression Space:</strong></p>
<ul>
<li>Begin with a randomly generated expression \( e_{\text{start}} \).</li>
<li>Apply a series of rewrite rules \( r_1, r_2, \ldots, r_n \) to generate a sequence of intermediate expressions \( e_1, e_2, \ldots, e_n \), ultimately arriving at a final expression \( e_{\text{end}} \).</li>
<li>Each step \( e_i \rightarrow e_{i+1} \) represents a proof step within a logical or mathematical framework.</li>
</ul>
</li>
<li>
<p><strong>Forming Theorems and Proofs:</strong></p>
<ul>
<li>The pair \( (e_{\text{start}}, e_{\text{end}}) \) represents a theorem, where \( e_{\text{start}} \) is the hypothesis and \( e_{\text{end}} \) is the conclusion.</li>
<li>The sequence of intermediate steps provides the proof for this theorem.</li>
<li>This method effectively generates theorems and their corresponding proofs by random exploration of the expression space.</li>
</ul>
</li>
<li>
<p><strong>Reversibility and Bidirectional Processes:</strong></p>
<ul>
<li>The reverse process can also be applied, starting with \( e_{\text{end}} \) and working backward to \( e_{\text{start}} \).</li>
<li>Intermediate steps that involve complex operations (e.g., integration) can often be reversed into simpler operations (e.g., differentiation).</li>
<li>This bidirectional approach ensures a rich dataset with varied difficulty levels for training models.</li>
</ul>
</li>
<li>
<p><strong>Automated Proof Generation:</strong></p>
<ul>
<li>By applying rewrite rules to random starting points and ending at random points, we automatically generate both theorems and proofs.</li>
<li>This method circumvents the traditional difficulty of finding a theorem to prove and then discovering its proof.</li>
<li>The randomness ensures a diverse set of theorems and proofs, capturing a wide range of logical and mathematical concepts.</li>
</ul>
</li>
</ol>
<p>This methodology allows us to systematically generate a large volume of training data for LLMs. By focusing on problems that are easy in one direction but complex in the other, we create a diverse dataset that captures a wide range of logical and mathematical challenges. This not only enhances the problem-solving capabilities of LLMs but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.</p>

          </p>
        </section>
      </div>

    
    


      
      <aside>
        
        
        
        <section style="margin-bottom: 1rem;">
          <h2 style="font-size: 1.5rem;">Related</h2>
          <div style="display: flex; flex-direction: column; gap: 1rem;">
            
            <a href="http://192.168.0.225:42999/projects/rpsdg/">RPSDG</a>
            
            <a href="http://192.168.0.225:42999/projects/digistar/">digistar</a>
            
            <a href="http://192.168.0.225:42999/ghprojects/digistar/">digistar</a>
            
            <a href="http://192.168.0.225:42999/projects/ngram-projections/">ngram-projections</a>
            
            <a href="http://192.168.0.225:42999/ghprojects/ngram-projections/">ngram-projections</a>
            
          </div>
        </section>
        

        
        
        <section style="margin-bottom: 1rem;">
          <h2 style="font-size: 1.5rem">Tags</h2>
          <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
            
            <a href="/tags/github" style="display: inline-block; padding: 0.5rem 1rem; background-color: #f0f0f0; border-radius: 20px; font-size: 1rem; text-decoration: none; color: #007acc; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
              GitHub
            </a>
            
            <a href="/tags/project" style="display: inline-block; padding: 0.5rem 1rem; background-color: #f0f0f0; border-radius: 20px; font-size: 1rem; text-decoration: none; color: #007acc; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
              project
            </a>
            
          </div>
        </section>
        
      </aside>

    </div>
  </div>

</section>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://192.168.0.225:42999/" >
    &copy;  metafunctor 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
