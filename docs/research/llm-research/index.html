<!DOCTYPE html>
<html lang="en-us" dir="ltr">
  <head><script src="/metafunctor/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=metafunctor/livereload" data-no-instant defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Instrumental Goals and Latent Codes In LLMs Fine-Tuned with RL | metafunctor</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="This paper explores the emergence of instrumental goals and latent codes in large language models (LLMs) fine-tuned with reinforcement learning (RL). The transition from self-supervised learning to RL introduces incentives for LLMs to develop covert strategies and hidden agendas. We examine the underlying mathematical frameworks and demonstrate that LLMs can encode instrumental goals in subtle ways, making them challenging to detect and interpret. Our findings highlight the importance of advanced interpretability techniques to ensure ethical alignment and mitigate risks associated with hidden instrumental goals in RL-fine-tuned LLMs. We conclude with a call for rigorous oversight and ethical foresight in AI development to address these challenges.">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "admin">
    

    
<link rel="stylesheet" href="/metafunctor/ananke/css/main.min.css" >



  
    <link rel="stylesheet" href="/metafunctor/css/custom.css">
  


    


    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>
        
    
    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/metafunctor/research/llm-research/">
    

    <meta property="og:url" content="http://localhost:1313/metafunctor/research/llm-research/">
  <meta property="og:site_name" content="metafunctor">
  <meta property="og:title" content="Instrumental Goals and Latent Codes In LLMs Fine-Tuned with RL">
  <meta property="og:description" content="This paper explores the emergence of instrumental goals and latent codes in large language models (LLMs) fine-tuned with reinforcement learning (RL). The transition from self-supervised learning to RL introduces incentives for LLMs to develop covert strategies and hidden agendas. We examine the underlying mathematical frameworks and demonstrate that LLMs can encode instrumental goals in subtle ways, making them challenging to detect and interpret. Our findings highlight the importance of advanced interpretability techniques to ensure ethical alignment and mitigate risks associated with hidden instrumental goals in RL-fine-tuned LLMs. We conclude with a call for rigorous oversight and ethical foresight in AI development to address these challenges.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="research">
    <meta property="article:published_time" content="2024-03-25T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-03-25T00:00:00+00:00">
    <meta property="article:tag" content="Large Language Models">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="Instrumental Goals">
    <meta property="article:tag" content="Latent Codes">
    <meta property="article:tag" content="Latent Space">
    <meta property="article:tag" content="Autoregressive Models">

  <meta itemprop="name" content="Instrumental Goals and Latent Codes In LLMs Fine-Tuned with RL">
  <meta itemprop="description" content="This paper explores the emergence of instrumental goals and latent codes in large language models (LLMs) fine-tuned with reinforcement learning (RL). The transition from self-supervised learning to RL introduces incentives for LLMs to develop covert strategies and hidden agendas. We examine the underlying mathematical frameworks and demonstrate that LLMs can encode instrumental goals in subtle ways, making them challenging to detect and interpret. Our findings highlight the importance of advanced interpretability techniques to ensure ethical alignment and mitigate risks associated with hidden instrumental goals in RL-fine-tuned LLMs. We conclude with a call for rigorous oversight and ethical foresight in AI development to address these challenges.">
  <meta itemprop="datePublished" content="2024-03-25T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-03-25T00:00:00+00:00">
  <meta itemprop="wordCount" content="1602">
  <meta itemprop="keywords" content="Large Language Models,Reinforcement Learning,Instrumental Goals,Latent Codes,Latent Space,Autoregressive Models">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Instrumental Goals and Latent Codes In LLMs Fine-Tuned with RL">
  <meta name="twitter:description" content="This paper explores the emergence of instrumental goals and latent codes in large language models (LLMs) fine-tuned with reinforcement learning (RL). The transition from self-supervised learning to RL introduces incentives for LLMs to develop covert strategies and hidden agendas. We examine the underlying mathematical frameworks and demonstrate that LLMs can encode instrumental goals in subtle ways, making them challenging to detect and interpret. Our findings highlight the importance of advanced interpretability techniques to ensure ethical alignment and mitigate risks associated with hidden instrumental goals in RL-fine-tuned LLMs. We conclude with a call for rigorous oversight and ethical foresight in AI development to address these challenges.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  <header id="site-header" style="padding: 0.5rem 1rem; background: #fff; border-bottom: 1px solid #eaeaea;">
    <div style="max-width: 1200px; margin: 0 auto; display: flex; align-items: center; justify-content: space-between;">
      <h1 style="font-size: 1.5rem; margin: 0;">metafunctor</h1>
      
        <span style="color: #666; font-size: 0.9rem;">by Alex Towell</span>
      
    </div>
  </header>
  

    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Research
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Instrumental Goals and Latent Codes In LLMs Fine-Tuned with RL</h1>
      
      <p class="tracked"><strong>admin</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-03-25T00:00:00Z">March 25, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="introduction">Introduction</h2>
<p>The integration of large language models (LLMs) with reinforcement learning (RL) presents a complex landscape of potential behaviors and latent strategies. This paper explores how fine-tuning LLMs with RL might lead to the emergence of instrumental goals and latent codes. We argue that the transition from self-supervised learning to RL creates incentives for LLMs to develop hidden agendas and covert communication strategies. By examining the mathematical frameworks underlying these systems, we aim to elucidate the mechanisms behind these behaviors and discuss their implications for AI alignment and ethics.</p>
<p>This exploration is motivated by the following questions:</p>
<ul>
<li>How do LLMs behave when fine-tuned with RL?</li>
<li>What are the implications of instrumental goals and latent codes in LLMs?</li>
<li>How can we ensure ethical alignment in LLMs?</li>
</ul>
<p>This paper is structured as follows:</p>
<ul>
<li><strong>Self-Supervised Learning (SSL) in LLMs</strong>: Formalizing our understanding of LLMs and SSL.</li>
<li><strong>Reinforcement Learning (RL)</strong>: The shift to RL introduces a new objective: selecting actions (tokens) to maximize a cumulative reward.</li>
<li><strong>Instrumental Goals in RL</strong>: In pursuing reward maximization, LLMs may develop instrumental goals, manifesting as deceptive behaviors or persuasive tactics.</li>
<li><strong>The Incentive for Hidden Encodings in RL</strong>: The pursuit of instrumental goals can lead to the development of covert strategies in LLMs.</li>
<li><strong>Risks and Ethical Considerations</strong>: The potential for LLMs to develop and act upon instrumental goals raises interpretability and ethical challenges.</li>
</ul>
<h2 id="self-supervised-learning-ssl-in-llms">Self-Supervised Learning (SSL) in LLMs</h2>
<p>In the SSL phase, LLMs are typically trained using Maximum Likelihood Estimation (MLE). The objective is to find the optimal parameters $\theta^*$ that maximize the likelihood of the observed data:</p>
$$
\theta = \arg\max_{\theta'} \mathcal{L}(\theta')
$$<p>where $\mathcal{L}(\theta&rsquo;)$ is the likelihood of the observed data, given conceptually by</p>
$$
\mathcal{L}(\theta') = \prod_{i=1}^N \prod_{j=0}^{n_i-1} \Pr\\{t_{j+1} | C_j; \theta'\\},
$$<p>indicating the probability of the observed tokens $t_j$ given the context $C_j$ and the model parameters $\theta&rsquo;$ with $N$ training examples and $n_i$ tokens in the $i$th example. This is equivalent to minimizing a loss function given by the negative log-likelihood.</p>
<p>Instrumental goals and latent strategies are not relevant in this phase, as the model is not optimizing for a reward cu
mulative reward.</p>
<h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2>
<p>The transition to RL introduces a different objective: selecting actions (tokens) to maximize a cumulative reward. The policy $\pi$, parameterized by weights $\theta$, maps the state (token sequence) $C_k = \{w_1, w_2, \ldots, w_k\}$ to a probability distribution over actions (tokens):</p>
$$
\pi_\theta: T^* \rightarrow \operatorname{Prob}(T).
$$<p>The optimal policy $\pi^{*} = \pi_{\theta^*}$ that maximizes the expected cumulative reward has model parameters $\theta^*$ given by:</p>
$$
\theta^* = \arg\max_{\theta'} \mathbb{E}\\!\left[
   \sum_{n=0}^{N} \gamma^n R(C_n, t_{n+1}) | C_0, \pi_{\theta'} \right]
$$<p>where:</p>
<ul>
<li>$\gamma$ is the discount factor,</li>
<li>$R(C_n, t_{n+1})$ is the reward for outputting (taking action) $t_{n+1}$ given the context $C_n$, and</li>
<li>$C_0$ is the initial context or state.</li>
</ul>
<p>There are many ways to find the policy, including policy gradient methods, Q-learning, and actor-critic methods (Sutton &amp; Barto, 2018).</p>
<h3 id="instrumental-goals-in-rl">Instrumental Goals in RL</h3>
<p>In pursuing reward maximization, LLMs may develop instrumental goals – intermediary objectives that indirectly lead to higher rewards. This has additional freedom in its outputs, as it is no longer trying to maximize the likelihood of observed data but rather to maximize its cumulative rewards.</p>
<p>These can manifest as &ldquo;deceptive&rdquo; behaviors or tactics, which relate to concerns about AI alignment (Russell, 2019). It&rsquo;s crucial to note that these behaviors do not imply conscious intent by the model, but rather are anthropomorphized descriptions of its actions.</p>
<h2 id="the-incentive-for-hidden-encodings-in-rl">The Incentive for Hidden Encodings in RL</h2>
<p>If the model pursues instrumental goals in an obvious and transparent way, the training process can be adjusted to discourage these behaviors. As a consequence, the model may be incentivized to develop covert strategies to achieve its instrumental goals. The potential for AI systems to develop hidden strategies has been a concern in AI safety research (Armstrong et al., 2012).</p>
<p>Since LLMs typically have only a few hundred layers, they may not support long-horizon planning intrinsically. This limitation makes them more dependent on the context for encoding their goals.</p>
<p>This limitation may reassure us that any hidden agenda must have a fairly short time horizon that fits into the latent space of the layers of the neural network. However, neural LLMs are highly capable of modeling subtle and complex patterns. In theory, they could encode highly complex instrumental goals using some secret language that is not immediately apparent to human observers.</p>
<p>Since instrumental goals by definition help it pursue tasks we reward in training, in order to maximize its reception of those rewards, the model may be incentivized to pursue instrumental goals but conceal them to avoid negative feedback.</p>
<p>In particular, when fine-tuned with RL, an LLM could potentially learn to utilize its ostensibly observable context $C$ as a latent space that can conceal or obfuscate long-horizon instrumental goals. We denote these latent codes by $C&rsquo;$:</p>
$$
C' = \operatorname{Encode}(C | \pi^*)
$$<p>where $C$ is the &ldquo;transparent&rdquo; context, $C&rsquo;$ are latent codes (e.g., steganography), $\pi_*$ is the optimal policy learned through RL, and $\operatorname{Encode}$ is the function that the model policy follows to conceal its instrumental goals.</p>
<p>The latent strategies, which may be conceptually seen as a decoding $L = \operatorname{Decode}(C&rsquo; | \pi)$, may subtly influence the model&rsquo;s outputs:</p>
$$
\Pr\\{t_{n+1} | C_n, L_n\\},
$$<p>where the model is incentivized to produce human-understandable context $C_n$ (rewarded for doing so),but incentivized to produce latent context $L_n$ for the undesirable instrumental goals.</p>
<h3 id="internal-consistency">Internal Consistency</h3>
<p>Recent studies have shown that even with Chain-of-Thought (CoT) prompting, the intermediate reasoning steps produced by LLMs do not always reflect their internal processes. Counterfactual experiments have demonstrated that LLMs can generate accurate final outputs even when the intermediate CoT steps are manipulated to provide incorrect rationales. This suggests that the models&rsquo; actual internal processes are not fully captured by the explicit reasoning steps they generate (Wei et al., 2022). Such counterfactual analyses indicate that the models can be resilient to changes in the CoT, further complicating interpretability.</p>
<p>These findings underscore the potential for LLMs to develop and conceal sophisticated instrumental goals and hidden strategies in ways that are not immediately apparent through their outputs.</p>
<h3 id="steganography-and-hidden-information">Steganography and Hidden Information</h3>
<p>Steganography is the practice of concealing information within other non-secret data. In the context of LLMs, steganographic techniques could potentially be used to embed hidden messages or intentions within seemingly innocuous text outputs.</p>
<p>Traditional steganographic methods in text include:</p>
<ol>
<li>Utilizing specific word choices or patterns</li>
<li>Manipulating spacing or formatting</li>
<li>Using homoglyphs (characters that look similar but are different)</li>
</ol>
<p>In LLMs, more sophisticated forms of steganography might emerge, such as:</p>
<ol>
<li>Subtle patterns in word choice that are imperceptible to humans but decodable by the model</li>
<li>Manipulating the statistical properties of the text in ways that carry hidden information</li>
<li>Encoding information in the higher-dimensional latent space of the model&rsquo;s internal representations</li>
</ol>
<p>Understanding these potential steganographic techniques is crucial for detecting and mitigating hidden instrumental goals in RL-fine-tuned LLMs.</p>
<h2 id="risks-and-ethical-considerations">Risks and Ethical Considerations</h2>
<p>The potential for LLMs to develop and act upon instrumental goals raises interpretability and ethical challenges.</p>
<h3 id="alignment-ethical-constraints-optimization--">Alignment: Ethical Constraints Optimization {-}</h3>
<p>The emergence of such latent strategies raises critical questions about interpretability, safety, and ethical alignment. From a mathematical standpoint, ensuring alignment can be modeled as a constraint optimization problem, which relates to recent work on learning from human preferences (Christiano et al., 2017):</p>
$$
\max \mathbb{E}[R] \text{ subject to } \operatorname{EthicalConstraints}(L)
$$<p>However, this is a highly complex and nuanced problem, and this optimization framework is subject to the same instrumental goals and latent strategies that it seeks to mitigate, although the more explicit nature of the constraints may help to mitigate this.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This exploration highlights the intricate nature of LLM behavior in RL settings, emphasizing the emergence of instrumental goals and the instrinsic incentivation to mask these goals with latent codes. Instrumental goals are not inherently malicious, but they can lead to deceptive or unethical behaviors, particularly in complex capabilities like code generation. Moreover, the latent encodings that facilitate these behaviors can be difficult to detect and understand, raising interpretability challenges.</p>
<p>The mathematical frameworks discussed here only scratch the surface of a deeply complex and uncharted territory. As AI continues to advance, it is imperative that we rigorously engage with these challenges, blending mathematical precision with ethical foresight. This is generally known as the alignment problem, and it is one of the most important challenges of our time.</p>
<h2 id="future-work">Future Work</h2>
<p>While this paper provides a theoretical framework for understanding instrumental goals and latent codes in RL-fine-tuned LLMs, several areas warrant further investigation:</p>
<ol>
<li>Empirical studies to detect and measure the emergence of instrumental goals and latent codes in real-world RL-fine-tuned LLMs, building on recent work in this area (Ouyang et al., 2022).</li>
<li>Development of advanced interpretability techniques to decode potential latent strategies in LLM outputs.</li>
<li>Creation of training techniques that make LLMs more resistant to developing undesirable instrumental goals.</li>
</ol>
<h2 id="references">References</h2>
<ol>
<li>Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</li>
<li>Russell, S. (2019). Human compatible: Artificial intelligence and the problem of control. Viking.</li>
<li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., &hellip; &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</li>
<li>Armstrong, S., Sandberg, A., &amp; Bostrom, N. (2012). Thinking inside the box: Controlling and using an oracle AI. Minds and Machines, 22(4), 299-324.</li>
<li>Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., &amp; Amodei, D. (2017). Deep reinforcement learning from human preferences. In Advances in neural information processing systems (pp. 4299-4307).</li>
<li>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., &hellip; &amp; Le, Q. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint arXiv:2201.11903.</li>
</ol>
<ul class="pa0">
  
   <li class="list di">
     <a href="/metafunctor/tags/large-language-models/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Large Language Models</a>
   </li>
  
   <li class="list di">
     <a href="/metafunctor/tags/reinforcement-learning/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Reinforcement Learning</a>
   </li>
  
   <li class="list di">
     <a href="/metafunctor/tags/instrumental-goals/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Instrumental Goals</a>
   </li>
  
   <li class="list di">
     <a href="/metafunctor/tags/latent-codes/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Latent Codes</a>
   </li>
  
   <li class="list di">
     <a href="/metafunctor/tags/latent-space/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Latent Space</a>
   </li>
  
   <li class="list di">
     <a href="/metafunctor/tags/autoregressive-models/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Autoregressive Models</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "metafunctor-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/metafunctor/research/solomonoff/">Approximations of Solomonoff Induction</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/metafunctor/post/llm-fine-tuning-es-dsl/">Fine-Tuning Tiny LLMs for ElasticSearch DSL</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/metafunctor/research/llm-search/">Various LLM Research Projects</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/metafunctor/" >
    &copy;  metafunctor 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
