<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Entropy Maps | metafunctor</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The PDF version of this post is available on GitHub.
The basic theory behind an entropy map is to map values in the domain to values in
the codomain by hashing to a prefix-free code in the codomain. We do not store
anything related to the domain, since we are simply hashing them, and a prefix
of that hash will be used as a code for a value in the codomain.">
    <meta name="generator" content="Hugo 0.139.3">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "admin">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>
        
    
    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/entropy-map/">
    

    <meta property="og:url" content="http://localhost:1313/posts/entropy-map/">
  <meta property="og:site_name" content="metafunctor">
  <meta property="og:title" content="Entropy Maps">
  <meta property="og:description" content="The PDF version of this post is available on GitHub.
The basic theory behind an entropy map is to map values in the domain to values in the codomain by hashing to a prefix-free code in the codomain. We do not store anything related to the domain, since we are simply hashing them, and a prefix of that hash will be used as a code for a value in the codomain.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-18T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-02-18T00:00:00+00:00">
    <meta property="og:image" content="http://localhost:1313/posts/entropy-map/featured.webp">

  <meta itemprop="name" content="Entropy Maps">
  <meta itemprop="description" content="The PDF version of this post is available on GitHub.
The basic theory behind an entropy map is to map values in the domain to values in the codomain by hashing to a prefix-free code in the codomain. We do not store anything related to the domain, since we are simply hashing them, and a prefix of that hash will be used as a code for a value in the codomain.">
  <meta itemprop="datePublished" content="2024-02-18T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-02-18T00:00:00+00:00">
  <meta itemprop="wordCount" content="2058">
  <meta itemprop="image" content="http://localhost:1313/posts/entropy-map/featured.webp">
  <meta itemprop="keywords" content="Entropy Map,Rate-Distortion,Bernoulli Map,Probabilistic Data Structure">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/posts/entropy-map/featured.webp">
  <meta name="twitter:title" content="Entropy Maps">
  <meta name="twitter:description" content="The PDF version of this post is available on GitHub.
The basic theory behind an entropy map is to map values in the domain to values in the codomain by hashing to a prefix-free code in the codomain. We do not store anything related to the domain, since we are simply hashing them, and a prefix of that hash will be used as a code for a value in the codomain.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  <header class="cover bg-top" style="background-image: url('/posts/entropy-map/featured.webp');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        metafunctor
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/social/" title="Social page">
              Social
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/ghprojects/" title="GitHub Projects page">
              GitHub Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="News page">
              News
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/probsets/" title="Problem Sets page">
              Problem Sets
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/research/" title="Research page">
              Research
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/search/" title="Search page">
              Search
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <div class="f2 f1-l fw2 white-90 mb0 lh-title">Entropy Maps</div>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        News
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Entropy Maps</h1>
      
      <p class="tracked">
        By <strong>admin</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-02-18T00:00:00Z">February 18, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>The PDF version of this post is available on <a href="https://github.com/queelius/bernoulli_data_type/tree/master/entropy-maps-paper/entropy-map.pdf">GitHub</a>.</p>
<p>The basic theory behind an entropy map is to map values in the domain to values in
the codomain by <em>hashing</em> to a prefix-free code in the codomain. We do not store
anything related to the domain, since we are simply hashing them, and a prefix
of that hash will be used as a code for a value in the codomain.</p>
<p>We actually allow for many different codes for each value in the codomain, so that,
for instance, a code for, say, the value <code>a</code> may be <code>00</code>, <code>01</code>, <code>10</code>, and <code>11</code>.
Notice that we can efficiently decode this as <code>a</code> if the hash is less than 4.</p>
$$
    \ell = -\sum_{y \in \mathcal{Y}} p_y \log_2 p_y,
$$<p>
which if we imagine sampling $x$ from $\mathcal{X}$ with $p_X$ and then mapping to
$y = f(x)$ and observing the sequence of $y$&rsquo;s, then the expected bit length is
the entropy of the sequence of $y$&rsquo;s. This is why we call it an entroy map.</p>
<p>If $\mathcal{X}$ is finite, then we can just imagine implicitly encoding the domain
and then for each value in the domain, storing the prefix-free code that it maps to,
which has an average bit length of $\ell$ and a total bit length of $|X| \ell$.</p>
<h2 id="rate-distortion-bernoulli-maps">Rate distortion: Bernoulli maps</h2>
<p>We can allow rate distortion, too, by failing to code for some of the elements
properly. For instance, a popular choice is when one of the values, say $y&rsquo;$, is
<em>extremely</em> common such that, for instance, $p_{y&rsquo;} &gt; .99$, then we can give it a
prefix-free code that sums to $p_{y&rsquo;}$ and then <em>not</em> code for it in the entropy map,
in which case it will, for some randomly selected $x \in \mathcal{X}$, be mapped to
$y&rsquo;$ with probability $p_{y&rsquo;}$ (which is sufficiently large, and can be made as
close to $1$ as desired if we wish to trade space for accuracy), and then for the
remaining values in the domain, code for them correctly (or also allow errors on
them, too, but only after trying to find correct codes for each of them).</p>
<h3 id="bernoulli-set-indicator-function">Bernoulli set-indicator function</h3>
$$
    1_{\mathcal{A}} : \mathcal{X} \to \\{0,1\\},
$$<p>
where $\mathcal{A} \subseteq \mathcal{X}$, and $\mathcal{X}$ is a very large set
(even infinite), then we may assign prefix-free codes for the codomain
value $1$ s.t. a priori, a random hash function hashes an element in $\mathcal{X}$
to a prefix-free code for $1$ with probability $\varepsilon$, where $\varepsilon$ is
very small, e.g., $2^{-10}$.</p>
<p>There are a countably infinite set of random hash functions which hash all
elements in $\mathcal{A} \subseteq \mathcal{X}$ to prefix-free codes for $1$ and
all other elements, $\mathcal{A}&rsquo; = \mathcal{X} \setminus \mathcal{A}$, to prefix codes
either for $0$ or $1$. If we are choosing a random hash function
that satisfies this property, then it is expected that $\varepsilon$ of the
elements in $\mathcal{A}&rsquo;$ will hash to a prefix-free code for $1$, and the
remaining $1 - \varepsilon$ will hash to a prefix-free code for $0$.</p>
<p>For any $x \in \mathcal{X}$, we can test if $1_{\mathcal{A}}(x) = 1$ by testing
if a prefix of $h(x)$ is a prefix-free code for $0$ or $1$, and if it is a code for
$0$, then we know that it is definitely not a member of $\mathcal{A}$, but if it is
a code for $1$, then it is a member of $\mathcal{A}$ with a false positive rate of
$\varepsilon$ and a true positive rate $1$, since a randomly drawn element
in $\mathcal{A}&rsquo;$ will hash to $0$ with probability $1 - \varepsilon$ and
any element in $\mathcal{A}$ will map to $1$ with probability $1$ (since we
explicitly chose a random hash function that hashes all of the elements in
$\mathcal{A}$ to a prefix-free code for $1$).</p>
<p>It is interesting to note that the entropy map initially frames the problem as a
compression problem, but we can also think of it as a rate-distortion problem.
Implicitly, in the above set-indicator function approximation, we are choosing to
minimize a loss function in which false negatives are much more costly than false
negatives, either because it is unlikely we will test a negative element for
membership, or because false positives are not nearly as costly as false negatives,
e.g., falsely thinking a rustling in the bushes is a tiger (false positive) is much
less costly than failing to notice a tiger in the bushes (false negative).</p>
<p>In either case, we call this set-indicator approximation a Bernoulli set-indicator
function, <code>bernoulli&lt;(set&lt;X&gt;, X) -&gt; bool&gt;{</code> $1_A$ <code>}</code>. This is the function that
is communicated, not the latent set-indicator function $1_A$.</p>
<p>A randomly chosen random hash function that satisfies (is conditioned on) the
property that it hashes all elements in $\mathcal{A}$ to a prefix-free code for $1$
has the confusion matrix in Table 1.</p>
<p>Table 1: Conditional distribution of Bernoulli set-indicator functions given latent
set-indicator function on $\mathcal{X} = \{a,b\}$</p>
<table>
  <thead>
      <tr>
          <th>latent/observed</th>
          <th>$1_\emptyset$</th>
          <th>$1_{\{a\}}$</th>
          <th>$1_{\{b\}}$</th>
          <th>$1_{\{a,b\}}$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$1_\emptyset$</td>
          <td>$(1-\varepsilon)^2$</td>
          <td>$(1-\varepsilon)\varepsilon$</td>
          <td>$(1-\varepsilon)\varepsilon$</td>
          <td>$\varepsilon^2$</td>
      </tr>
      <tr>
          <td>$1_{{a}}$</td>
          <td>$0$</td>
          <td>$1-\varepsilon$</td>
          <td>$0$</td>
          <td>$\varepsilon$</td>
      </tr>
      <tr>
          <td>$1_{{b}}$</td>
          <td>$0$</td>
          <td>$0$</td>
          <td>$1-\varepsilon$</td>
          <td>$\varepsilon$</td>
      </tr>
      <tr>
          <td>$1_{{a,b}}$</td>
          <td>$0$</td>
          <td>$0$</td>
          <td>$0$</td>
          <td>$1$</td>
      </tr>
  </tbody>
</table>
<p>We see that the constraint of no false negatives generates a confusion matrix
with a lot of zeros. If we observe <code>bernoulli&lt;set&lt;X&gt;,X) -&gt; bool&gt;{</code>$1_{\{a\}}$<code>}</code>, then the latent
set-indicator function is either $1_{\emptyset}$ or $1_{\{a\}}$. Since $\varepsilon$
is very small, we can be fairly certain that the latent set-indicator function is
$1_{{a}}$.</p>
<p>What is the total degrees-of-freedom for a confusion matrix of this type?</p>
<p>Table 2: Confusion matrix with maximum degrees-of-freedom</p>
<table>
  <thead>
      <tr>
          <th>latent/observed</th>
          <th>$1_\emptyset$</th>
          <th>$1_{\{a\}}$</th>
          <th>$1_{\{b\}}$</th>
          <th>$1_{\{a,b}}$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$1_\emptyset$</td>
          <td>$p_{1 1}$</td>
          <td>$p_{1 2}$</td>
          <td>$p_{1 3}$</td>
          <td>$1-p_{1 1}-p_{1 2}-p_{1 3}$</td>
      </tr>
      <tr>
          <td>$1_{{a}}$</td>
          <td>$p_{2 1}$</td>
          <td>$p_{2 2}$</td>
          <td>$p_{2 3}$</td>
          <td>$1-p_{2 1}-p_{2 2}-p_{2 3}$</td>
      </tr>
      <tr>
          <td>$1_{{b}}$</td>
          <td>$p_{3 1}$</td>
          <td>$p_{3 2}$</td>
          <td>$p_{3 3}$</td>
          <td>$1-p_{3 1}-p_{3 2}-p_{3 3}$</td>
      </tr>
      <tr>
          <td>$1_{{a,b}}$</td>
          <td>$p_{4 1}$</td>
          <td>$p_{4 2}$</td>
          <td>$p_{4 3}$</td>
          <td>$1-p_{4 1}-p_{4 2}-p_{4 3}$</td>
      </tr>
  </tbody>
</table>
<p>We see that there are $4 \times (4 - 1) = 12$ degrees-of-freedom for the confusion
matrix in Table 2. For the confusion matrix in Table 1, we have $1$ degrees-of-freedom, since
we have $1$ parameter, $\varepsilon$.</p>
<p>The degree-of-freedom is one way to think about the complexity of a model. The more
degrees-of-freedom, the more complex the model. The more complex the model, the more
data we need to estimate the parameters of the model, although frequently we already
know the parameters of the model, since it may have been specified as a part of the
algrorithm that generated the Bernoulli approximation.</p>
<p>The confusion matrix in Tables 1 and 2 represent the conditional distribution of the
Bernoulli set-indicator function given the latent set-indicator function, which we
denote by <code>bernoulli&lt;set&lt;X&gt;,X) -&gt; bool&gt;</code>.</p>
<h3 id="boolean-bernoulli-as-constant-function">Boolean Bernoulli as constant function</h3>
<p>How many functions are there of type <code>() -&gt; bool</code>? There are two, <code>true</code> and <code>false</code>.
That is, there are $|\{true, false\}|^{|\{1\}|} = 2^1 = 2$ functions.</p>
<p>So, we can also think of Boolean values as functions of type <code>() -&gt; bool</code>. Then,
when we apply the Bernoulli model <code>bernoulli&lt;() -&gt; bool&gt;</code>, we get the same result
as before.</p>
<p>Table 3: Confusion matrix for Bernoulli model applied to Boolean values</p>
<table>
  <thead>
      <tr>
          <th>latent/observed</th>
          <th><code>true</code></th>
          <th><code>false</code></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>true</code></td>
          <td>$p_{1 1}$</td>
          <td>$1-p_{1 1}$</td>
      </tr>
      <tr>
          <td><code>false</code></td>
          <td>$1-p_{2 2}$</td>
          <td>$p_{2 2}$</td>
      </tr>
  </tbody>
</table>
<p>This confusion matrix has a maximum of two degrees-of-freedom, since there are two
parameters, $p_{1 1}$ and $p_{2 2}$, since we have the constraint that the sum of the
probabilities in each row is $1$.</p>
<p>In the binary symmetric channel, $p_{1 1} = p_{2 2}$:</p>
<p>Table 4: Confusion matrix for Bernoulli model applied to Boolean values</p>
<table>
  <thead>
      <tr>
          <th>latent/observed</th>
          <th><code>true</code></th>
          <th><code>false</code></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>true</code></td>
          <td>$p$</td>
          <td>$1-p$</td>
      </tr>
      <tr>
          <td><code>false</code></td>
          <td>$1-p$</td>
          <td>$p$</td>
      </tr>
  </tbody>
</table>
<h3 id="conditional-distribution-of-latent-function-given-observed-function">Conditional distribution of latent function given observed function</h3>
<p>Once we <em>have</em> an observation, say <code>bernoulli&lt;set&lt;X&gt;,X) -&gt; bool&gt;{x}</code>, what does the
confusion matrix tell us? Let&rsquo;s abstract the problem a bit so we can focus on
deriving the result.</p>
<p>Let $X$ and $Y$ be random variables.
Assume that $P(X = x | Y = y)$ is difficult to compute, but $P(Y = y | X = x)$ is
easy. (This is the case for the confusion matrix. We know the conditional
distribution of the observed set-indicator function given the latent set-indicator
function, but we want to know the conditional distribution of the latent
set-indicator function given the observed set-indicator function, which is not
directly available.)</p>
$$
P(X = x | Y = y) = \frac{P(Y = y | X = x) P(X = x)}{P(Y = y)}
$$<p>So, to compute $P(X = x | Y = y)$, we need to know two additional things. First, what
is $P(X = x)$? This is usually a prior. If we know something about the distribution
of $X$, then encode that information in $P(X = x)$, otherwise we can use an
uninformed prior, e.g., assign a uniform probability to each possibility.</p>
$$
P(Y = y) = \sum_{x'} P(Y = y | X = x') P(X = x')
$$$$
P(X = x | Y = y) = \frac{P(Y = y | X = x)}{\sum_{x'} P(Y = y | X = x')}
$$<p>So, let&rsquo;s replace $X$ with the latent set-indicator function <code>x</code> and $Y$ with the
observed <code>bernoulli&lt;(set&lt;X&gt;,X&gt; -&gt; bool&gt;{y}</code>. Then, we can compute the conditional
distribution of the latent <code>x</code> given the observed <code>bernoulli&lt;(set&lt;X&gt;,X&gt; -&gt; bool&gt;{y}</code>
by looking at the confusion matrix in Table 2 and picking out the specific row
and column of interest and then normalizing by the sum of the column.</p>
$$
p_{k|2} = \frac{p_{k 2}}{\sum_{j=1}^4 p_{j 2}},
$$<p>
where $k$ is the row corresponding to the latent set-indicator function of interest
and we conditioning on column $2$, the index for the observed set-indicator
function $1_{\{a\}}$.</p>
$$
p_{k|i} = \frac{p_{k i}}{\sum_{j=1}^4 p_{j i}},
$$<p>
where $k$ is the row corresponding to the latent set-indicator function of interest
and we are conditioning on column $i$, the index for the observed set-indicator
function. If we do this for the four possible observed set-indicator functions for
Table 2 (confusion matrix with only one degree-of-freedom), we get Table 5.</p>
<p>Table 5: Conditional probability of latent set-indicator function given observed
set-indicator function</p>
<table>
  <thead>
      <tr>
          <th>observed/latent</th>
          <th>$1_\emptyset$</th>
          <th>$1_{\{a\}}$</th>
          <th>$1_{\{b\}}$</th>
          <th>$1_{\{a,b\}}$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$1_\emptyset$</td>
          <td>$1$</td>
          <td>$0$</td>
          <td>$0$</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$1_{{a}}$</td>
          <td>$\varepsilon/(1+\varepsilon)$</td>
          <td>$1/(1+\varepsilon)$</td>
          <td>$0$</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$1_{{b}}$</td>
          <td>$\varepsilon/(1+\varepsilon)$</td>
          <td>$0$</td>
          <td>$1/(1+\varepsilon)$</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$1_{{a,b}}$</td>
          <td>$\varepsilon^2/(1+\varepsilon)^2$</td>
          <td>$\varepsilon/(1+\varepsilon)^2$</td>
          <td>$\varepsilon/(1+\varepsilon)^2$</td>
          <td>$1/(1+\varepsilon)^2$</td>
      </tr>
  </tbody>
</table>
<p>The conditional distribution in Table 5 is one way to think about the uncertainty of the
latent set-indicator function given the observed set-indicator function. The entropy
is another way to think about the uncertainty, but we will not compute it here.</p>
<p>We see that when we observe the empty set for a Bernoulli model in which
false negatives are not possible, then we know for certain that the latent
set-indicator function is $1_\emptyset$. However, when we observe $1_{\{a\}}$,
we are uncertain about the latent set-indicator function. We know that it is
either $1_{\emptyset}$ or $1_{\{a\}}$, but we do not know which one it is. Since
$\varepsilon$ is small, it is more much likely to be $1_{\{a\}}$ than $1_{\emptyset}$,
though. A similar argument holds for the other two observed set-indicator functions.</p>
<h2 id="algorithms">Algorithms</h2>
<p>The simplest algorithm is a one-level hash function evaluation, where we hash the
domain values concatenated with some bit string $b$ such that when we decode the values
$h(x + b)$, $x \in \mathcal{X}$, we get a prefix-free code for $y = f(x)$.</p>
<h3 id="two-level-hash-function-evaluation">Two-level hash function evaluation</h3>
<p>The more practical solution is a two-level hash scheme. First, we hash each
$x \in \mathcal{X}$ concatented with the same bit string $b$, same as before. However,
we use this hash value to index into a hash table $H$ at, say, index $j$. Now, we
choose a bit string for $H[j]$ for each $x \in \mathcal{X}$ that hashes to $j$ such
that $f(x) = \text{decode}(h(x + H[j]))$.</p>
<p>This way, we can keep the probability
$p_j = \prod_x \Pr\{ f(x) = \text{decode}(h(x + H[j]))\}$ for each $x$ that hashes
to $j$ more or less constant, independent of the size of the codomain $\mathcal{X}$,
by choosing an appropriately-sized hash table $H$.</p>
<p>Since each decoding is an independent Bernoulli trial, we see that
the probability that a particular $x$ that hashes to $j$ is decoded correctly is
the number of hashes that are a prefix-free code for $f(x)$ divided by the total number of
hashes (e.g., an $N$ bit hash function has $2^N$ possible values).</p>
<h3 id="oblivious-entropy-maps">Oblivious entropy maps</h3>
<p>An <em>oblivious</em> entropy map is just an entropy map where the hash function is
applied to trapdoors of $\mathcal{X}$ and the prefix-free codes for $\mathcal{Y}$
have no rythm or reason to them, e.g., a random selection of hash values for each
value in $\mathcal{Y}$.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "metafunctor-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  metafunctor 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
