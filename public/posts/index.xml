<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>News on metafunctor</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in News on metafunctor</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Advancing Mathematical Reasoning in AI: Introducing Reverse-Process Synthetic Data Generation</title>
      <link>http://localhost:1313/posts/rpsdg/</link>
      <pubDate>Tue, 25 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rpsdg/</guid>
      <description>&lt;p&gt;Check out the (early) project and source code on &lt;a href=&#34;https://github.com/queelius/RPSDG&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This paper introduces a methodology for generating high-quality, diverse training data for Language Models (LMs) in complex problem-solving domains. Our approach, termed &amp;ldquo;Reverse-Process Synthetic Data Generation&amp;rdquo; (RPSDG), inverts traditionally difficult problems to create an abundance of training examples with known solutions,&#xA;e.g., symbolically taking the deriative of a function, $f \mapsto f&amp;rsquo;$, versus solving antiderivatives of $f&amp;rsquo;$. By automating the generation of problems of graduating difficulty, we create datasets that enable process-supervised training of LLMs. We demonstrate the efficacy of this method for training mathematical reasoning. Our results show significant improvements in LLMs&amp;rsquo; problem-solving capabilities, particularly in areas requiring multi-step reasoning and creative insights. This methodology not only enhances model performance but also provides a framework for generating explainable AI solutions, as the step-by-step problem-solving process is inherent in the training data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SLUUG: Demystifying Large Language Models (LLMs) on Linux: From Theory to Application</title>
      <link>http://localhost:1313/posts/gave-a-presentation-for-sluug-about-llms/</link>
      <pubDate>Fri, 23 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gave-a-presentation-for-sluug-about-llms/</guid>
      <description>Gave a talk for the St. Louis Unix Users Group (SLUUG) about Large Language Models (LLMs) on Linux titled &amp;lsquo;Demystifying Large Language Models (LLMs) on Linux: From Theory to Application&amp;rsquo;.</description>
    </item>
    <item>
      <title>Master&#39;s project: Reliability Estimation in Series Systems</title>
      <link>http://localhost:1313/posts/masters-stats-siue-proj/</link>
      <pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/masters-stats-siue-proj/</guid>
      <description>I presented my master&amp;rsquo;s project in October 2023. It was titled &amp;lsquo;Reliability Estimation in Series Systems: Maximum Likelihood Techniques for Right-Censored and Masked Failure Data&amp;rsquo;.</description>
    </item>
    <item>
      <title>Fine-Tuning Tiny LLMs for ElasticSearch DSL</title>
      <link>http://localhost:1313/posts/llm-fine-tuning-es-dsl/</link>
      <pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/llm-fine-tuning-es-dsl/</guid>
      <description>I am creating a tiny LLM for ElasticSearch DSL as a proof of concept.</description>
    </item>
    <item>
      <title>The Bernoulli Model: A Probabilistic Framework for Data Structures and Types</title>
      <link>http://localhost:1313/posts/bernoulli-boolean-model/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bernoulli-boolean-model/</guid>
      <description>This blog post introduces the Bernoulli Model, a framework for understanding probabilistic data structures and incorporating uncertainty into data types, particularly Boolean values. It highlights the model&amp;rsquo;s utility in optimizing space and accuracy in data representation.</description>
    </item>
    <item>
      <title>AlgoTree: Advanced Tree Structure Algorithms in Python</title>
      <link>http://localhost:1313/posts/algotree-post/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/algotree-post/</guid>
      <description>Introducing AlgoTree, a Python package for working with various tree-like data structures, offering flexible APIs and powerful utilities.</description>
    </item>
    <item>
      <title>Infini-gram: LLM-scale *-gram models</title>
      <link>http://localhost:1313/posts/infini-gram/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/infini-gram/</guid>
      <description>&lt;p&gt;Recently, I watched a presentation on &lt;a href=&#34;https://huggingface.co/spaces/liujch1998/infini-gram&#34;&gt;Infini-grams&lt;/a&gt;, which utilize a suffix array to avoid precomputing $n$-grams and allow for arbitrary context lengths, up to a suffix that is found in the training data.&lt;/p&gt;&#xA;&lt;p&gt;This sparked my interest as I had worked on a similar project for a LLM talk I gave for SLUUG at &lt;a href=&#34;https://www.stllinux.org/&#34;&gt;https://www.stllinux.org&lt;/a&gt; (see my GitHub repo &lt;a href=&#34;https://github.com/queelius/sluug-talk-llm&#34;&gt;https://github.com/queelius/sluug-talk-llm&lt;/a&gt; and the video fo the talk at &lt;a href=&#34;https://www.sluug.org/resources/presentations/media/2024/STLLINUX/2024-02-22_STLLINUX_2560x1440.mp4&#34;&gt;https://www.sluug.org/resources/presentations/media/2024/STLLINUX/2024-02-22_STLLINUX_2560x1440.mp4&lt;/a&gt;) where in part of the talk I demonstrated arbitrary-size $n$-grams using a recursive dictionary to store synthetic training data prefix counts to implement a crude expression tree evaluator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy Maps</title>
      <link>http://localhost:1313/posts/entropy-map/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/entropy-map/</guid>
      <description>&lt;p&gt;The PDF version of this post is available on &lt;a href=&#34;https://github.com/queelius/bernoulli_data_type/tree/master/entropy-maps-paper/entropy-map.pdf&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The basic theory behind an entropy map is to map values in the domain to values in&#xA;the codomain by &lt;em&gt;hashing&lt;/em&gt; to a prefix-free code in the codomain. We do not store&#xA;anything related to the domain, since we are simply hashing them, and a prefix&#xA;of that hash will be used as a code for a value in the codomain.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Boolean Algebra Over Trapdoors</title>
      <link>http://localhost:1313/posts/trapdoor-boolean-model/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/trapdoor-boolean-model/</guid>
      <description>&lt;p&gt;This project is available on &lt;a href=&#34;https://github.com/queelius/bernoulli_data_type/boolean-algebra-trapdor/&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;boolean-algebra&#34;&gt;Boolean Algebra&lt;/h2&gt;&#xA;&lt;p&gt;A Boolean algebra is a mathematical structure that captures the properties of logical operations and sets. Formally, it is defined as a 6-tuple $(B, \land, \lor, \neg, 0, 1)$, where&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$B$ is a set of elements,&lt;/li&gt;&#xA;&lt;li&gt;$\land$ ($\rm{and}$) and $\lor$ $\rm{or}$ are binary operations on $B$,&lt;/li&gt;&#xA;&lt;li&gt;$\neg$ ($\rm{not}$) is a unary operation on $B$,&lt;/li&gt;&#xA;&lt;li&gt;$0$ and $1$ are elements of $B$, often referred to as the minimum and maximum elements, respectively.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;These components must satisfy certain axioms, including closure of $B$ under the operations, commutativity, associativity, distributivity, and the existence of identity and complement elements [1].&lt;/p&gt;</description>
    </item>
    <item>
      <title>Uses and limits of abstractions</title>
      <link>http://localhost:1313/posts/uses-and-limits-of-abstractions/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/uses-and-limits-of-abstractions/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m been thinking about the power and limitations of abstractions in our&#xA;understanding of the world. This blog post is from a chat I had with a ChatGPT,&#xA;which can be found &lt;a href=&#34;https://chat.openai.com/share/f298898b-9787-48f4-8959-c8cd04eb98b4&#34;&gt;here&lt;/a&gt;&#xA;and &lt;a href=&#34;https://chat.openai.com/share/0d33ab33-0664-4b25-b1c2-22864b28db48&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;I&amp;rsquo;m not sure if this is a good blog post, but I&amp;rsquo;m posting it anyway. It&amp;rsquo;s remarkable&#xA;how quickly you can slap stuff like this together, and I&amp;rsquo;m not sure this is&#xA;saying anything valuable, particularly since it only required a bit of prompting&#xA;from me.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Working memory as an inductive bias</title>
      <link>http://localhost:1313/posts/working-memory-as-an-inductive-bias/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/working-memory-as-an-inductive-bias/</guid>
      <description>&lt;p&gt;This blog post is from a chat I had with a ChatGPT,&#xA;which can be found &lt;a href=&#34;https://chat.openai.com/share/f298898b-9787-48f4-8959-c8cd04eb98b4&#34;&gt;here&lt;/a&gt;&#xA;and &lt;a href=&#34;https://chat.openai.com/share/0d33ab33-0664-4b25-b1c2-22864b28db48&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;I&amp;rsquo;m not sure if this is a good blog post, but I&amp;rsquo;m posting it anyway. It&amp;rsquo;s remarkable&#xA;how quickly you can slap stuff like this together, and I&amp;rsquo;m not sure this is&#xA;saying anything valuable, particularly since it only required a bit of prompting&#xA;from me.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;hr&gt;&#xA;&lt;img src=&#34;./featured.png&#34; style=&#34;width: 200px; float: left; margin: 10px;&#34;&gt;&#xA;&lt;h2 id=&#34;working-memory-as-an-inductive-bias&#34;&gt;Working memory as an inductive bias&lt;/h2&gt;&#xA;&lt;p&gt;Human cognitive abilities, while remarkable, are bounded. Our working memory can effectively hold and process only a limited amount of information at once. Cognitive psychology often references the &amp;ldquo;magic number seven&amp;rdquo;, suggesting that most adults can hold between five and nine items in their working memory. This constraint necessitates the use of abstractions in order to understand complex systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Problem set solutions</title>
      <link>http://localhost:1313/posts/problem-sets/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/problem-sets/</guid>
      <description>&lt;p&gt;I have a fairly broad interest in problem-solving, from problems in statistics&#xA;to algorithms. Over the years, I&amp;rsquo;ve accumulated a collection of problem sets,&#xA;which I will be refining and posting here every now and then.&lt;/p&gt;&#xA;&lt;p&gt;You can find these problem sets on various topics &lt;a href=&#34;https://metafunctor.com/#prob-sets&#34;&gt;here&lt;/a&gt;.&#xA;It&amp;rsquo;s accessible through the menu bar as well. Please note that while I don&amp;rsquo;t&#xA;claim expertise in all areas, these materials may still provide valuable insights.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using GPT-4 to Build a Simple HTML File Search Interface</title>
      <link>http://localhost:1313/posts/made-chatgpt-to-my-saved-conversations/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/made-chatgpt-to-my-saved-conversations/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;This blog post written by GPT-4. See conservation with GPT-4 that built it&#xA;&lt;a href=&#34;https://chatgpt.metafunctor.com/browse/gpt4/chat-gpt-python-search-html-files.html&#34;&gt;here&lt;/a&gt;.&#xA;The interface to the browse/search is &lt;a href=&#34;https://chatgpt.metafunctor.com/&#34;&gt;here&lt;/a&gt;.&#xA;It&amp;rsquo;s really not fancy, but I&amp;rsquo;ve never had much of an interest in doing this&#xA;kind of front-end work, but GPT-4 makes it pretty easy.&#xA;Btw, I just realize that I think I forgot to reindex database before submitting,&#xA;so it doesn&amp;rsquo;t seem to be able to find that conversation except by browsing to it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Survey: Accelerating Critical Section Execution with Asymmetric Multi-Core Architectures (2009)</title>
      <link>http://localhost:1313/posts/2022-05-13-acs/</link>
      <pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2022-05-13-acs/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;$$&#xA;  \operatorname{speedup}(N) = \frac{1}{1 - P + \frac{P}{N}},&#xA;$$&lt;p&gt;&#xA;where $\operatorname{speedup}$ is the total execution time of a&#xA;single-threaded version of a program divided by the total execution time&#xA;an $N$-threaded implementation of the same program.a&lt;/p&gt;&#xA;&lt;p&gt;Accordingly, the asymptotic speedup of a program as a function of $N$ is&#xA;determined by the $1-P$ term, i.e., $\operatorname{speedup}(N)$ goes to&#xA;$1/(1-P)$ as $N$ goes to infinity. Thus, even if 99% of the time the&#xA;program can be ran in parallel, the maximum possible&#xA;$\operatorname{speedup}$ is only $1/(1-0.99) = 100$. Thus, to achieve&#xA;scalable performance with increasing thread counts, the sequential&#xA;portion of code must be aggressively reduced no matter how &lt;em&gt;small&lt;/em&gt; it&#xA;is.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multiprocessor synchronization: tournament-Peterson lock</title>
      <link>http://localhost:1313/posts/2012-03-05-tournament-lock/tournament_lock/</link>
      <pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2012-03-05-tournament-lock/tournament_lock/</guid>
      <description>&lt;p&gt;Multiprocessor synchronization is a notoriously tricky subject matter.&#xA;Unlike with a single thread of execution, in a shared-resource system, where&#xA;resources are shared among multiple independent processors, we must think very&#xA;hard about how the critical sections where such shared resources are accessed.&lt;/p&gt;&#xA;&lt;p&gt;Definition: Peterson&amp;rsquo;s algorithm is a concurrent programming algorithm for mutual&#xA;exclusion that allows two or more processes to share a single-use resource without&#xA;conflict, using only shared memory for communication.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Review: A Symbolic Representation of Time Series, with Implications for Streaming Algorithms</title>
      <link>http://localhost:1313/posts/2012-02-04-sax/cs584_sax_review/</link>
      <pubDate>Sat, 30 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2012-02-04-sax/cs584_sax_review/</guid>
      <description>&lt;p&gt;In [1], the authors present a method for constructing a symbolic (nominal) representation for real-valued time series data. A symbolic representation is desirable because then it becomes possible to use many of the effective algorithms that require symbolic representation, like hashing and Markov models.&lt;/p&gt;&#xA;&lt;p&gt;The authors claim that one of the most useful time series operations is measuring the similarity between two time series data sets. To do this on the original time series, the Euclidean distance formula can be used. Therefore, for a time series transformation to be useful, distance measures applied to the corresponding transformations should provide some guaranteed lower bound on the &lt;code&gt;true&lt;/code&gt; distance. This is a basic requirement for almost all time series algorithms in data mining. Non-symbolic transformations like Discrete Fourier Transform (DFT) and Piecewise Aggregate Approximation (PAA) models have this lower-bounding property. However, the authors claim no previously proposed symbolic representations do, which limits their usefulness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DIY Home Lab</title>
      <link>http://localhost:1313/posts/diy-home-lab/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/diy-home-lab/</guid>
      <description>&lt;h2 id=&#34;why-a-home-lab&#34;&gt;Why a Home Lab?&lt;/h2&gt;&#xA;&lt;p&gt;In this day and age where cloud computing seems to be the de facto standard for most&#xA;computing needs, the question arises - why go through the hassle of building and&#xA;maintaining a home lab?&lt;/p&gt;&#xA;&lt;h3 id=&#34;home-improvement-projects&#34;&gt;Home Improvement Projects&lt;/h3&gt;&#xA;&lt;p&gt;During the Covid lockdown, my wife found a new passion for home improvement projects and restoring old furniture, transforming a spare room into a beautiful home office. Her inspiration came from Francine Jay&amp;rsquo;s book, &amp;ldquo;The Joy of Less,&amp;rdquo; which explores the beauty of minimalism and self-sufficiency. It gave her a creative outlet that made our home a&#xA;more functional and plesant space witout breaking the piggy bank.&#xA;I have a similar attitude towards computing and, in particular, home servers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rank-orderd Encrypted Search</title>
      <link>http://localhost:1313/posts/rank-ordered-encrypted-search/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/rank-ordered-encrypted-search/</guid>
      <description>&lt;h1 id=&#34;rank-ordered-encrypted-search&#34;&gt;Rank-ordered Encrypted Search&lt;/h1&gt;&#xA;&lt;p&gt;Consider a map from a set of keys to a set of values, where the keys are the search keys&#xA;or relations between search keys, and the values are the information about the search keys which may be used to compute rank-ordering, such as frequency or proximity information.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Encrypted search&lt;/strong&gt; enables untrusted systems to obliviously search documents on behalf of authorized users, where the search query, the confidential documents being searched over, and the search results are oblivious values.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
