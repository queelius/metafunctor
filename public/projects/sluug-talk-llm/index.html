<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>sluug-talk-llm | metafunctor</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Stars: 1 | Forks: 0 | Open Issues: 0
Languages: Jupyter Notebook, Python, Shell
Contributors

queelius (23 commits)

README

marp: true
#theme: uncover
math: mathjax
SLUUG Talk: Large Language Models
This repository contains the slides and code for the talk:

Demystifying Large Language Models (LLMs) on Linux: From Theory to Application

It was given for the St. Louis Unix Users Group (SLUUG) on 2024/2/22 @ 6:30 PM CST.


SLUUG: https://www.stllinux.org/ :link:


Meetup: https://www.meetup.com/saint-louis-unix-users-group/events/290697932/ :link:">
    <meta name="generator" content="Hugo 0.139.2">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Alex Towell">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    }
  };
</script>
        
    
    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/projects/sluug-talk-llm/">
    

    <meta property="og:url" content="http://localhost:1313/projects/sluug-talk-llm/">
  <meta property="og:site_name" content="metafunctor">
  <meta property="og:title" content="sluug-talk-llm">
  <meta property="og:description" content="Stars: 1 | Forks: 0 | Open Issues: 0
Languages: Jupyter Notebook, Python, Shell
Contributors queelius (23 commits) README marp: true #theme: uncover math: mathjax SLUUG Talk: Large Language Models This repository contains the slides and code for the talk:
Demystifying Large Language Models (LLMs) on Linux: From Theory to Application It was given for the St. Louis Unix Users Group (SLUUG) on 2024/2/22 @ 6:30 PM CST.
SLUUG: https://www.stllinux.org/ :link:
Meetup: https://www.meetup.com/saint-louis-unix-users-group/events/290697932/ :link:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2024-02-09T09:45:34+00:00">
    <meta property="article:modified_time" content="2024-02-09T09:45:34+00:00">
    <meta property="article:tag" content="GitHub">
    <meta property="article:tag" content="Project">

  <meta itemprop="name" content="sluug-talk-llm">
  <meta itemprop="description" content="Stars: 1 | Forks: 0 | Open Issues: 0
Languages: Jupyter Notebook, Python, Shell
Contributors queelius (23 commits) README marp: true #theme: uncover math: mathjax SLUUG Talk: Large Language Models This repository contains the slides and code for the talk:
Demystifying Large Language Models (LLMs) on Linux: From Theory to Application It was given for the St. Louis Unix Users Group (SLUUG) on 2024/2/22 @ 6:30 PM CST.
SLUUG: https://www.stllinux.org/ :link:
Meetup: https://www.meetup.com/saint-louis-unix-users-group/events/290697932/ :link:">
  <meta itemprop="datePublished" content="2024-02-09T09:45:34+00:00">
  <meta itemprop="dateModified" content="2024-02-09T09:45:34+00:00">
  <meta itemprop="wordCount" content="2658">
  <meta itemprop="keywords" content="GitHub,Project">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="sluug-talk-llm">
  <meta name="twitter:description" content="Stars: 1 | Forks: 0 | Open Issues: 0
Languages: Jupyter Notebook, Python, Shell
Contributors queelius (23 commits) README marp: true #theme: uncover math: mathjax SLUUG Talk: Large Language Models This repository contains the slides and code for the talk:
Demystifying Large Language Models (LLMs) on Linux: From Theory to Application It was given for the St. Louis Unix Users Group (SLUUG) on 2024/2/22 @ 6:30 PM CST.
SLUUG: https://www.stllinux.org/ :link:
Meetup: https://www.meetup.com/saint-louis-unix-users-group/events/290697932/ :link:">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        metafunctor
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/social/" title="Social page">
              Social
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About Me page">
              About Me
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/ghprojects/" title="GitHub Projects page">
              GitHub Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/posts/" title="News page">
              News
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/probsets/" title="Problem Sets page">
              Problem Sets
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/research/" title="Research page">
              Research
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/search/" title="Search page">
              Search
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
<section class="project-section" style="padding: 2rem; background-color: #f9f9f9;">
  <div class="container" style="max-width: 1200px; margin: 0 auto;">
    
    <div style="display: grid; grid-template-columns: 2fr 1fr; gap: 2rem;">
      
      
      <div>
        
        <header style="margin-bottom: 2rem;">
          <h1 style="font-size: 2.5rem; font-weight: 700; color: #333;">sluug-talk-llm</h1>
          <p style="color: #555; font-size: 1.1rem; margin-bottom: 0.5rem;">
             | February 9, 2024 | <a href="mailto:" style="color: #007acc;"></a>
          </p>

          
          <div style="overflow: hidden;">
            
            <p style="font-size: 1.25rem; color: #666;"></p>
          </div>
        </header>

        
        
        <section style="margin-bottom: 1rem;">
          <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
            
            <a href="https://github.com/queelius/sluug-talk-llm" target="_blank" style="padding: 0.75rem 1.5rem; background-color: #007acc; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
              GitHub
            </a>
            
            
            <a href="https://twitter.com/intent/tweet?text=sluug-talk-llm&url=http%3a%2f%2flocalhost%3a1313%2fprojects%2fsluug-talk-llm%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #1DA1F2; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-twitter"></i>
            </a>

            
            <a href="https://www.linkedin.com/sharing/share-offsite/?url=http%3a%2f%2flocalhost%3a1313%2fprojects%2fsluug-talk-llm%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #0077B5; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-linkedin"></i>
            </a>

            
            <a href="https://www.facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fprojects%2fsluug-talk-llm%2f" target="_blank" style="padding: 0.75rem .75rem; background-color: #1877F2; color: white; border-radius: 5px; text-decoration: none; transition: background-color 0.3s ease;">
              <i class="fab fa-facebook"></i>
            </a>
          </div>
        </section>
        

        
        <section style="margin-bottom: 3rem;">
          <h2 style="font-size: 1.75rem; font-weight: 600; color: #007acc;">Project Description</h2>
          <p style="font-size: 1.1rem; color: #555; line-height: 1.8;">
            <p><strong>Stars</strong>: 1 | <strong>Forks</strong>: 0 | <strong>Open Issues</strong>: 0</p>
<p><strong>Languages</strong>: Jupyter Notebook, Python, Shell</p>
<h2 id="contributors">Contributors</h2>
<ul>
<li>queelius (23 commits)</li>
</ul>
<h2 id="readme">README</h2>
<hr>
<h2 id="math-mathjax">marp: true
#theme: uncover
math: mathjax</h2>
<h1 id="sluug-talk-large-language-models">SLUUG Talk: Large Language Models</h1>
<p>This repository contains the slides and code for the talk:</p>
<ul>
<li>Demystifying Large Language Models (LLMs) on Linux: From Theory to Application</li>
</ul>
<p>It was given for the St. Louis Unix Users Group (SLUUG) on 2024/2/22 @ 6:30 PM CST.</p>
<ul>
<li>
<p>SLUUG: <a href="https://www.stllinux.org/">https://www.stllinux.org/</a> :link:</p>
</li>
<li>
<p>Meetup: <a href="https://www.meetup.com/saint-louis-unix-users-group/events/290697932/">https://www.meetup.com/saint-louis-unix-users-group/events/290697932/</a> :link:</p>
</li>
</ul>
<hr>
<h1 id="alex-towell">Alex Towell</h1>
<ul>
<li>
<p><a href="mailto:lex@metafunctor.com">lex@metafunctor.com</a> :email:</p>
</li>
<li>
<p><a href="https://metafunctor.com">https://metafunctor.com</a> :link:</p>
</li>
<li>
<p><a href="https://github.com/queelius">https://github.com/queelius</a> :rocket:</p>
</li>
<li>
<p><a href="https://twitter.com/queelius">https://twitter.com/queelius</a> :bird:</p>
</li>
<li>
<p>Important URLs for this talk:</p>
<ul>
<li>Talk link: <a href="https://github.com/queelius/sluug-talk-llm">https://github.com/queelius/sluug-talk-llm</a> :link:</li>
<li>Colab notebook on n-gram model: <a href="https://colab.research.google.com/drive/1ak4kOtbIQGXE5kuhhGTd55xu4qRpeZd7?usp=sharing">https://colab.research.google.com/drive/1ak4kOtbIQGXE5kuhhGTd55xu4qRpeZd7?usp=sharing</a></li>
<li>ElasticSearch NLQ demo (down): <a href="http://lab.metafunctor.com:6789">http://lab.metafunctor.com:6789</a> (API: <a href="http://lab.metafunctor.com:6789/docs">http://lab.metafunctor.com:6789/docs</a>)</li>
</ul>
</li>
</ul>
<hr>
<h1 id="outline-of-talk">Outline of Talk</h1>
<ul>
<li>
<p>Theoretical Background</p>
</li>
<li>
<p>Go over a simple language model</p>
<ul>
<li>$n$-gram model (Jupyter Notebook)</li>
<li>Easy to understand and helps us understand some aspects of LLMs.</li>
</ul>
</li>
<li>
<p>Show an application of LLMs:</p>
<ul>
<li>Try to make a database search API intelligent (NLP)
with small LLMs.</li>
</ul>
</li>
<li>
<p>Open Discussion</p>
</li>
</ul>
<hr>
<h1 id="good-old-fashioned-ai-gofai">Good-Old-Fashioned AI (GOFAI)</h1>
<ul>
<li>
<p>Find a way to symbolically represent the problem and
then use logic or rules to solve it.</p>
<ul>
<li>
<p>Programming :computer:</p>
</li>
<li>
<p>Rule-based systems :robot:</p>
</li>
<li>
<p>First-order logic</p>
</li>
</ul>
</li>
<li>
<p>LLMs are <em>good</em> at using these tools. :hammer:</p>
<ul>
<li>Integrate Prolog with LLM tool-use to help with planning and reasoning?</li>
</ul>
</li>
</ul>
<hr>
<h3 id="reductive-reasoning">Reductive Reasoning</h3>
<p><img src="./symbolic.png" alt="bg left height:3.2in"></p>
<p>GOFAI works for a lot of problems we care about:</p>
<ul>
<li>Filter everything through our small working memory.
<ul>
<li>Inductive bias: Makes assumptions about the world.</li>
<li>Help us generalize out-of-distribution. :brain:</li>
</ul>
</li>
<li>Take big problems and break down into simpler problems.</li>
<li>Solve simpler problems and combine.</li>
</ul>
<hr>
<h1 id="limits-of-gofai">Limits of GOFAI</h1>
<p>Many problems are hard to break down into simpler parts.</p>
<ul>
<li>
<p>Whole greater than the sum of its parts.</p>
</li>
<li>
<p>Too complex to solve reductively.</p>
<ul>
<li>We can&rsquo;t program computers to do it. :shrug:</li>
<li>Identifying cats in pictures? :cat:</li>
<li>
<blockquote>
<p>The hard problems are easy and the easy problems are hard.
&ndash; Steven Pinker</p>
</blockquote>
</li>
<li>Playing with legos is hard but multivariate calculus is easy (for a computer).</li>
</ul>
</li>
</ul>
<hr>
<h1 id="how-do-our-brains-work">How Do Our Brains Work?</h1>
<p><img src="./subsymbolic.png" alt="bg left height:3.5in"></p>
<p>Brains programmed by evolution to survive in a complex world.</p>
<ul>
<li>It&rsquo;s a prediction engine: it learns to predict the world.</li>
<li>The unconscious mind is not limited by a small &ldquo;working memory&rdquo;</li>
<li>It can do things we don&rsquo;t understand how to do.</li>
<li>Brain is a black box. (See: <em>Interpretable ML</em>)</li>
</ul>
<hr>
<h1 id="machine-learning">Machine Learning</h1>
<p>:bulb: Let&rsquo;s have the computer learn from data.</p>
<ul>
<li>
<p>Since the real world is too complex, let&rsquo;s have the computer learn from data like we do.</p>
</li>
<li>
<p>There are three main types of learning.</p>
<ul>
<li>Supervised Learning (SL)</li>
<li>Unsupervised Learning ðŸ”¥</li>
<li>Reinforcement Learning (RL) ðŸ’£</li>
</ul>
</li>
<li>
<p><em>Spoiler</em>: LLMs use self-supervised learning (SSL) and RL (RLHF).</p>
</li>
</ul>
<hr>
<h2 id="type-of-learning-1-supervised-learning">Type of Learning (1): Supervised Learning</h2>
<p><strong>Learning from labeled data</strong>. We have some input and output data, and we want to learn how to map the input to the output.</p>
<ul>
<li>
<p>Given an (unknown) function $f$ and a set of input-output pairs $(x, f(x))$, learn a function $\hat{f}$ that approximates $f$ on the input-output pairs.</p>
</li>
<li>
<p>E.g., classification: $f$ : [ :cat: or :dog: ] â†¦ { :cat: , :dog: }.</p>
<ul>
<li>Use $\hat{f}$ to predict :cat: or :dog: for new images.</li>
</ul>
</li>
<li>
<p>Easiest problem to solve in ML. But: limited by data.</p>
</li>
<li>
<p><strong>Fine-Tuning</strong> LLMs is supervised learning: improve it on specific labeled tasks.</p>
</li>
</ul>
<hr>
<h2 id="type-of-learning-2-unsupervised-learning">Type of Learning (2): Unsupervised Learning</h2>
<p><strong>No labeled data</strong>. Learn the underlying structure of the data.</p>
<ul>
<li>
<p>Clustering: Grouping similar data points. (See: <em>RAG</em>)</p>
</li>
<li>
<p>Dimensionality Reduction: Learn <em>efficient</em> representations of the data.</p>
<ul>
<li>Very hard and one of the most important problems in ML.</li>
</ul>
</li>
<li>
<p>Density Estimation: Stochastic estimate of process that generated the observed data. Say the process generates $(x, y)$ pairs and we estimate its density $\Pr(x, y)$.</p>
<ul>
<li>Classification (supervised): $\Pr(y|x) = \Pr(x, y) / \Pr(x)$</li>
</ul>
</li>
<li>
<p><strong>Pre-training LLMs</strong> is like unsupervised learning. Learn a good representation and probability distribution of the <em>raw</em> text using self-supervised learning (SSL).</p>
</li>
</ul>
<hr>
<h2 id="final-type-of-learning-3-reinforcement-learning">Final Type of Learning (3): Reinforcement Learning</h2>
<p>This is an agentic approach to learning. Agent interacts with environment and learns from the rewards it receives.</p>
<ul>
<li><em>Goal</em>: maximize the expected sum of rewards.</li>
<li><em>Spoiler</em>: Agentic frameworks that include LLMs as a prediction component is a very active area of research.</li>
<li><code>Prediction + Search = Planning</code>
<ul>
<li>Counterfactual reasoning</li>
</ul>
</li>
<li>Hypothesis: <code>Compression = Prediction = Intelligence</code></li>
<li>Big reason a lot of people are excited about Sora.
<ul>
<li>Has everyone seen the Sora videos?</li>
<li>&ldquo;Intuitive&rdquo; world simulation (embedded in the weights of a giant NN).</li>
</ul>
</li>
</ul>
<hr>
<h1 id="early-failures-in-ml">Early Failures in ML</h1>
$$
(x_1, x_2, \ldots, x_n),
$$<p>
$n$ extremely large and each $x_i$ some complex object.</p>
<ul>
<li>
<p>Overfitting, curse of dimensionality, lack of data/compute.</p>
</li>
<li>
<p>To combat lack of data/compute, clever solutions developed.</p>
</li>
<li>
<p>Many of these methods are no longer around.</p>
<blockquote>
<p>&ldquo;The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.&rdquo;
&ndash; Richard Sutton&rsquo;s Bitter Lesson</p>
</blockquote>
</li>
</ul>
<hr>
<h1 id="neural-networks">Neural Networks</h1>
<p><img src="./NN.jpg" alt="bg left height:6in"></p>
<p>Neural Networks (NN) are one the solutions that stuck around.</p>
<ul>
<li>It fell out of favor for a while, but it&rsquo;s back.</li>
<li>Universal function approximator.
<ul>
<li>Can learn to represent any function.</li>
<li>But: need a lot of data to do so and be difficult to train.</li>
</ul>
</li>
<li>NNs seem to scale to as much data and compute as we can throw at them.</li>
</ul>
<hr>
<h1 id="inductive-bias">Inductive Bias</h1>
<p>Observations may have an infinite set of hypothesis that are compatible with the data.</p>
<ul>
<li>
<p><strong>Inductive Bias</strong>: The set of assumptions that the model makes about the data.</p>
</li>
<li>
<p><strong>Occam&rsquo;s Razor</strong>: choose the simplest hypothesis that is compatible with the data. (See <em>Solomonoff Induction</em>.)</p>
</li>
<li>
<p>Generalizing out-of-distribution (OOD) from inputs not in the training data.</p>
</li>
<li>
<p><strong>Problem</strong>: We are almost <em>always</em> out-of-distribution.</p>
<ul>
<li>Except in toy problems (see: early successes)</li>
</ul>
</li>
<li>
<p>Good inductive biases are necessary for generalization.</p>
</li>
<li>
<p><strong>No Free Lunch Theorem</strong>: No model is optimal for all tasks.</p>
</li>
</ul>
<hr>
<h1 id="era-of-deep-learning">Era of Deep Learning</h1>
<p><img src="./imagenet.png" alt="bg left height:4in"></p>
<p>One of the hardest parts is learning sample efficient representation of the data.</p>
<ul>
<li>
<p>Layers of NN learn progressively higher-level representations: <code>Pixels -&gt; Edges -&gt; Objects</code></p>
</li>
<li>
<p>AlexNet (2012) was the first to show that deep learning could work well on large-scale datasets.</p>
</li>
</ul>
<hr>
<h1 id="era-of-deep-learning-cont">Era of Deep Learning (cont.)</h1>
<p><img src="./image-4.png" alt="bg left height:4.1in"></p>
<p>DNNs (feed-forward) learn little circuit programs that can generate parts of the training data. (Image stolen from Jeff Dean&rsquo;s slides.)</p>
<ul>
<li>
<p>Hundreds of layers: can learn pretty complicated programs.</p>
</li>
<li>
<p>(What a human can do in a half a second, a DNN can do?)</p>
</li>
</ul>
<hr>
<h1 id="era-of-generative-ai">Era of Generative AI</h1>
<p><img src="image-5.png" alt="bg left height:4.1in"></p>
<p>Generative AI &ldquo;reverses&rdquo; the arrows
- Image to text, image to image, etc.</p>
<ul>
<li>They learn something about the data generating process (DGP).</li>
<li>They have completely changed our expectations of what computers can do.</li>
</ul>
<hr>
<h1 id="era-of-generative-ai-cont">Era of Generative AI (cont.)</h1>
<p>We now have computers that can see, hear, understand, and generate all of these things.</p>
<p>Let&rsquo;s go look at <strong>Sora</strong>: generative video, or world(s) simulator?</p>
<ul>
<li>
<p><strong>Scaling</strong>: And increasing the scale (data, compute) increase their capabilities. See: Scaling laws.</p>
<ul>
<li>Need a lot more <em>compute</em>.</li>
<li>It&rsquo;s going to get wild(er).</li>
<li>Hypothesis: <code>Prediction = Compression = Intelligence</code>.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="large-language-models-llms">Large Language Models (LLMs)</h1>
<p>Autoregressive (AR) models learn a probability distribution over training data by using self-supervised learning (SSL):</p>
$$
\Pr(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T \Pr(x_t | x_1, \ldots, x_{t-1})
$$<ul>
<li>This is hard to learn, but with enough data and compute, a lot seems possible.</li>
<li>LLMs have a nice advantage since language is designed to have a very low dimensionality and have a high signal to noise ratio.
<ul>
<li>Representation learning is easier in language than in other domains.
<ul>
<li>Still learns representations (<code>word2vec</code>)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Language</strong> represents much of the things that humans care and think about, so learning to predict it is a kind of general intelligence. (See: Sparks of AGI by Microsoft)</li>
</ul>
<hr>
<h1 id="sampling-from-llms">Sampling from LLMs</h1>
<p>There are many different ways to sample from LLMs and change the behavior of the model.</p>
<ul>
<li><strong>Temperature</strong>: Rescaling the logits before applying the softmax function.
<ul>
<li>$T = 1$: estimates the probability distribution.</li>
<li>$T &lt; 1$: reduces randomness, i.e., more predictable outputs.</li>
<li>$T &gt; 1$: increases randomness, i.e., more unpredictable outputs.</li>
</ul>
</li>
</ul>
<p>Good for controlling <em>exploitation</em> vs <em>exploration</em> if repeatedly sampling from the model to generate new or different outputs.</p>
<ul>
<li>
<p><strong>Top-k and Top-p Sampling</strong>: Choose the top-$k$ or top-$p$ tokens and sample from them.</p>
</li>
<li>
<p><strong>Beam Search</strong>: Explore multiple paths and sample based on that joint probability.</p>
</li>
</ul>
<hr>
<h2 id="prompting-strategies">Prompting Strategies</h2>
<p>Early models were very sensitive to the prompt.</p>
<ul>
<li>Makes sense, they were trained to generate the data.</li>
<li>If you condition on crazy data, you get crazy outputs.</li>
</ul>
$$
\Pr(\text{more crazy}|\text{crazy})
$$<p>Various prompting strategies have been developed to help the model generate more reliable outputs:</p>
<ul>
<li>Chain-of-thought (CoT)</li>
<li>Tree-of-thought (ToT)</li>
<li>and so on&hellip;</li>
</ul>
<hr>
<h1 id="llm-overview">LLM Overview</h1>
<p>Basic idea: train a model to predict the next token in a sequence of tokens.</p>
<ul>
<li>
<p><strong>Task</strong>: Given a sequence of tokens, predict the next token.</p>
<ul>
<li>Pre-Train model to learn raw data distribution using SSL.</li>
<li>Fine-tune model to a specific dataset that is more relevant to a task.</li>
<li>RLHF model to bias it to produce outputs that people prefer.</li>
</ul>
</li>
<li>
<p><strong>Goal</strong>: Enable the generation of new data points for a given task.</p>
</li>
</ul>
<hr>
<h2 id="ood-generalization">OOD Generalization</h2>
<p>At inference, outputs are almost always out-of-distribution (OOD).</p>
<ul>
<li>
<p><em>In-Context Learning</em>: Transformers seem to be pretty good at generalizing from data that was not seen during training.</p>
</li>
<li>
<p>Learning to predict the next token when the data is sufficiently complicated may require a general kind of intelligence.</p>
</li>
<li>
<p><em>Causal inductive bias</em>: The model is biased to predict the next token based on the evidence of the previous tokens.</p>
</li>
</ul>
<p><em>Example</em>: &ldquo;Based on all the previous evidence, I conclude that the murderer is ___&rdquo;. To do this well, it seems you must be able to reason about the evidence.</p>
<hr>
<h2 id="naive-n-gram-model-ar-over-bytes">Naive N-Gram Model (AR) Over Bytes</h2>
<p><img src="./naive-ngram/expr_tree.png" alt="bg left height:6in"></p>
<p>We consider an AR-LM over bytes (256 tokens):</p>
<ul>
<li><em>Algorithmic training data</em>: Partial expression trees.
<ul>
<li><em>Sparse</em> markov chain of order $O(256^n)$ states.</li>
</ul>
</li>
<li>Analyze how well model predicts the next token given the context.</li>
<li>How well does model capture the underlying process?
<ul>
<li><em>Spoiler</em>: It doesn&rsquo;t do well.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="implementation-notes">Implementation Notes</h3>
<ul>
<li>
<p>We represent our $n$-gram model as a dictionary of dictionaries:</p>
<ul>
<li>Outer dictionary is indexed by context.</li>
<li>Inner dictionary is indexed by next token.</li>
<li>Each <code>token | context</code> maps frequency in training data.</li>
</ul>
</li>
<li>
<p>This is simple model and simple data</p>
<ul>
<li>Hopefully, exploring its properties can help us understand LLMs.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="colab">Colab</h2>
<p>Let&rsquo;s go to the notebook.</p>
<ul>
<li>If you want to follow along, Colab is available at: <a href="https://colab.research.google.com/drive/1ak4kOtbIQGXE5kuhhGTd55xu4qRpeZd7?usp=sharing">https://colab.research.google.com/drive/1ak4kOtbIQGXE5kuhhGTd55xu4qRpeZd7?usp=sharing</a> :link:</li>
<li>See my GitHub: <a href="https://github.com/queelius/sluug-talk-llm">https://github.com/queelius/sluug-talk-llm</a> :link:</li>
</ul>
<hr>
<h3 id="colab-comments">Colab Comments</h3>
<p><em>Inductive Bias</em>: Throwing away oldest bytes is a strong inductive bias:</p>
<ul>
<li>Not necessarily true that the next byte is less dependent on the oldest bytes.</li>
</ul>
<p><em>Generative Model</em>: generate text by starting with a any context and then sampling from the probability distribution for that context to get the next token.</p>
<ul>
<li>Repeat until we have generated the desired number of tokens.</li>
<li>Same way LLMs work (but they work well).</li>
</ul>
<hr>
<h3 id="colab-advantages-of-our-model">Colab: Advantages of Our Model</h3>
<p>Our model has some advantages compared to AR-LLMs. Since we simply <em>store</em> the data:</p>
<ul>
<li>Easy to implement.</li>
<li>Easy to make it a lifelong learner. Store <em>more data</em>.</li>
</ul>
<hr>
<h3 id="colab-disadvantages-of-our-model">Colab: Disadvantages of Our Model</h3>
<p>But, compared to more sophisticated models, they have huge disadvantages:</p>
<ul>
<li>
<p>$n$-gram model is not able to capture long-range dependencies in the data.</p>
<ul>
<li>Number of states grows exponentially with the order of the model.</li>
<li>It cannot scale to large contexts, and therefore cannot understand
nuances in the data.</li>
</ul>
</li>
<li>
<p>$n$-gram model does not generalize out-of-distribution very well.</p>
<ul>
<li>Since language is a high-dimensional space, <em>most</em> contexts have never been seen before.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="colab-conclusion">Colab: Conclusion</h3>
<p>Key concept in ML: A <em>good</em> model <em>compresses</em> the data.</p>
<ul>
<li>
<p>There is a notion that <em>compression</em> is a proxy for <em>understanding</em>.</p>
</li>
<li>
<p>Take a <em>physics simulation</em>: we don&rsquo;t need to store the position and velocity of every particle.</p>
<ul>
<li>We can just store the starting conditions and then let the laws of physics play out.</li>
<li>Not perfect, but perfect prediction impossible.
<ul>
<li>Only need to predict it well enough to make informed decisions.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Prediction = compression = intelligence</code></p>
<ul>
<li>The brain may be a good example of this.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="finite-state-machines">Finite State Machines</h2>
<p>We can view AR-LMs as finite state machines (if deterministic) otherwise Markov chains without loss of generality.</p>
<ul>
<li>Computers are FSMs, just very large ones.</li>
<li>LLMs are also very large FSMs.</li>
</ul>
<p><a href="https://www.lesswrong.com/posts/7qSHKYRnqyrumEfbt">https://www.lesswrong.com/posts/7qSHKYRnqyrumEfbt</a></p>
<ul>
<li>Thus, AR-LLMs are differentiable computers that can learn from examples.</li>
</ul>
<hr>
<h1 id="tool-use">Tool-Use</h1>
<p>There is a lot of training data about how to use tools and APIs.  ðŸ”¨</p>
<ul>
<li>
<p>Large LLMs like GPT-4 do a good job predicting when and how they should use tools.</p>
</li>
<li>
<p>Let&rsquo;s go over to the ElasticSearch NLQ demo. ðŸ”¦</p>
</li>
</ul>
<hr>
<h1 id="elasticsearch-demo">ElasticSearch Demo</h1>
<ul>
<li>
<p>Making all endpoints on the internet and UIs intelligent with small and fast LLMs.</p>
</li>
<li>
<p>As a trial, we are using ElasticSearch as a backend to enable natural language queries (NLQs) on ElasticSearch indexes (databases).</p>
</li>
<li>
<p>Key take-aways: GPT-4 / GPT-3.5 are good, small LLMs not quite there yet.</p>
<ul>
<li>
<p>We have some ways to possibly improve them though. More on that later.</p>
</li>
<li>
<p>And, of course, today&rsquo;s large models are tomorrow&rsquo;s small models.</p>
<ul>
<li>Desperately need more compute!</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="elasticsearch-what-is-it">ElasticSearch: What Is It?</h2>
<ul>
<li>An open source, scalable search engine.</li>
<li>Supports complex queries, aggregations, and full-text search.</li>
<li>Can be difficult to use.</li>
<li>Suppose we have <code>articles</code> index with <code>author</code> and <code>title</code> fields and want to count the number of articles by author:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;size&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;aggs&#34;</span>: {
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;articles_by_author&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">&#34;terms&#34;</span>: { <span style="color:#f92672">&#34;field&#34;</span>: <span style="color:#e6db74">&#34;author&#34;</span> }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 id="fastapi-what-is-it-and-how-do-we-use-it">FastAPI: What Is It and How Do We Use It?</h2>
<ul>
<li>A fast web framework for building APIs with Python.</li>
<li>We are trying two things:
<ul>
<li>Using ElasticSearch backend for storage and search.</li>
<li>Using LLMs to convert natural language queries (NLQ) to ElasticSearch queries.</li>
</ul>
</li>
<li>We expose a single endpoint <code>/{index}/nlq</code> that takes an
index and an NLQ and returns a result from ElasticSearch.
<ul>
<li>Hopefully the result is useful!</li>
</ul>
</li>
<li>Later, remind me to open my firewall to allow access.</li>
</ul>
<hr>
<h2 id="structure-of-indexes">Structure of Indexes</h2>
<p>I populated ElasticSearch with a two example indexes:</p>
<ul>
<li>
<p><code>articles</code>: A simple index with <code>author</code>, <code>title</code>, and &lsquo;publication_date&rsquo; fields.</p>
</li>
<li>
<p><code>gutenberg</code>: A more complex index with <code>author</code>, <code>publication_date</code>,<code>title</code>, and <code>content</code> fields.</p>
</li>
</ul>
<hr>
<h2 id="code">Code</h2>
<p>Let&rsquo;s look at some code. We&rsquo;ll switch to the code editor. There are
a few files we need to look at:</p>
<ul>
<li><code>main.py</code>: The FastAPI app. We can probe it using the Swagger UI at <code>http://lab.metafunctor.com:6789/docs</code>.</li>
<li>There is a crude frontend at <code>http://lab.metafunctor.com:6789/</code>.
<ul>
<li>I made the frontend by chatting with ChatGPT-4. By chatting, I mean asked two ill-formed questions and copied its code blocks.</li>
<li>See this link: <a href="https://chat.openai.com/share/9c95ba2e-94e7-4d9f-ae89-095357fc39bd">https://chat.openai.com/share/9c95ba2e-94e7-4d9f-ae89-095357fc39bd</a></li>
</ul>
</li>
<li><code>nlq.py</code>: The module that handles the NLQ to ElasticSearch query conversion.</li>
<li><code>examples.py</code>: A crude example database. We&rsquo;ll talk about this in a bit.</li>
</ul>
<hr>
<h2 id="issues">Issues</h2>
<ul>
<li>GPT-4 is good at converting NLQs to ElasticSearch queries, but it&rsquo;s slow and expensive to use at scale.
<ul>
<li>We only need to use an LLM for a relatively narrow task.</li>
<li>Maybe we don&rsquo;t need the full power of GPT-4?</li>
</ul>
</li>
<li>Small LLMs, like <code>llama2</code>, did poorly on converting NLQs to ElasticSearch queries.</li>
</ul>
<hr>
<h2 id="idea-1-use-gpt-4-to-teach-smaller-models">Idea #1: Use GPT-4 to &ldquo;Teach&rdquo; Smaller Models</h2>
<p>Use GPT-4 to generate high-quality examples for smaller LLMs.</p>
<ul>
<li>
<p>Feed examples into the context of the small LLM to do In-Context Learning (ICL).</p>
<ul>
<li><strong>ICL</strong>: a model can generalize to new NLQs</li>
</ul>
</li>
<li>
<p>How? Every now and then, use GPT-4 to do the task and store its NLQ to ElasticSearch query in example database.</p>
</li>
<li>
<p>Let&rsquo;s look at the <code>examples.py</code> code.</p>
<ul>
<li>DB is just a Python <code>{}</code> that doesn&rsquo;t persist.</li>
<li>Didn&rsquo;t have the time to use a proper database.
<ul>
<li>Ironic considering this is all about how to use ElasticSearch!</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="issues-1">Issues</h3>
<p>The smaller models, like <code>llama2:13b</code> , do not seem to generalize
from the examples very well.</p>
<ul>
<li>They often do better without &ldquo;polluting&rdquo; their context
with too much information.</li>
<li>More tweaking? Or are these small models simply not up to the task.</li>
</ul>
<hr>
<h2 id="idea-2-rag-retrieval-augmented-generation">Idea #2: RAG (Retrieval-Augmented Generation)</h2>
<p>Maybe the smaller models need to be fed with more <em>relevant</em> examples. Use RAG to find relevant examples for the given index and NLQ :bulb:</p>
<ul>
<li>
<p>Send the context through a language model to get a dense representation.</p>
</li>
<li>
<p>Store the representation of the examples in the database.</p>
</li>
<li>
<p>Find examples closest to the context of the NLQ and sample from them.</p>
</li>
<li>
<p>Insert the high-quality examples into the context of the small LLM to do ICL.</p>
</li>
</ul>
<hr>
<h2 id="idea-3-fine-tuning">Idea #3: Fine-Tuning</h2>
<ul>
<li>
<p>Fine-tune the smaller models on far more high-quality examples.</p>
</li>
<li>
<p>Small LLMs won&rsquo;t have to In-Context Learn as much.</p>
</li>
<li>
<p>See my GitHub repo: <a href="https://github.com/queelius/elasticsearch-lm">https://github.com/queelius/elasticsearch-lm</a></p>
<ul>
<li>
<p>Its README has a lot of verbiage.</p>
</li>
<li>
<p>I just ran it through GPT-4 and didn&rsquo;t bother to edit it much.</p>
</li>
</ul>
</li>
</ul>
<hr>
<h1 id="discussion">Discussion</h1>
<p><img src="./image-6.png" alt="bg left height:3.5in"></p>

          </p>
        </section>
      </div>

    
    


      
      <aside>
        
        
        
        <section style="margin-bottom: 1rem;">
          <h2 style="font-size: 1.5rem;">Related</h2>
          <div style="display: flex; flex-direction: column; gap: 1rem;">
            
            <a href="http://localhost:1313/ghprojects/sluug-talk-llm/">sluug-talk-llm</a>
            
            <a href="http://localhost:1313/projects/elasticsearch-lm/">elasticsearch-lm</a>
            
            <a href="http://localhost:1313/ghprojects/elasticsearch-lm/">elasticsearch-lm</a>
            
            <a href="http://localhost:1313/projects/video-playlist-manager/">video-playlist-manager</a>
            
            <a href="http://localhost:1313/ghprojects/video-playlist-manager/">video-playlist-manager</a>
            
          </div>
        </section>
        

        
        
        <section style="margin-bottom: 1rem;">
          <h2 style="font-size: 1.5rem">Tags</h2>
          <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
            
            <a href="/tags/github" style="display: inline-block; padding: 0.5rem 1rem; background-color: #f0f0f0; border-radius: 20px; font-size: 1rem; text-decoration: none; color: #007acc; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
              GitHub
            </a>
            
            <a href="/tags/project" style="display: inline-block; padding: 0.5rem 1rem; background-color: #f0f0f0; border-radius: 20px; font-size: 1rem; text-decoration: none; color: #007acc; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
              project
            </a>
            
          </div>
        </section>
        
      </aside>

    </div>
  </div>

</section>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  metafunctor 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
